{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAI Task 1 Description : Group Chewbhan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the United Nations, one in three people worldwide do not have access to safe drinking water. Unsafe water is a leading risk factor for death, especially at low incomes, and is one of the world's largest health and environmental problems. Groundwater pollution occurs when pollutants are released into the ground and make their way down into groundwater. While water contamination can occur from naturally occurring contaminants, such as arsenic or fluoride, common causes of water pollution are on-site sanitation systems, effluent from wastewater treatment plants, petrol filling stations or agricultural fertilizers.\n",
    "\n",
    "In order to prevent outbreaks and incidents of water poisonings, detecting ground-water contamination is crucial. Geostatistics has often utilized the Gaussian Process (GP) to model the spatial pattern of pollutant concentrations in the ground. Usually, a data point in 2D represents a geological well where a sample was taken from a bore hole to measure concentration of pollutants.\n",
    "\n",
    "In the following task, we will use Gaussian Process regression (or a similar method) in order to model groundwater pollution, and try to predict the concentration of pollutants at previously unmeasured wells (points).\n",
    "\n",
    "In order to perfom this task we followed the GPytorch Regression Tutorial :[https://docs.gpytorch.ai/en/v1.2.0/examples/01_Exact_GPs/Simple_GP_Regression.html]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "import gpytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training data\n",
    "train_x_name = \"train_x.csv\"\n",
    "train_y_name = \"train_y.csv\"\n",
    "\n",
    "train_x = np.loadtxt(train_x_name, delimiter=',')\n",
    "train_y = np.loadtxt(train_y_name, delimiter=',')\n",
    "\n",
    "# load the test dateset\n",
    "test_x_name = \"test_x.csv\"\n",
    "test_x = np.loadtxt(test_x_name, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constant for Cost function\n",
    "THRESHOLD = 0.5\n",
    "W1 = 1\n",
    "W2 = 20\n",
    "W3 = 100\n",
    "W4 = 0.04\n",
    "\n",
    "\n",
    "def cost_function(true, predicted):\n",
    "    \"\"\"\n",
    "        true: true values in 1D numpy array\n",
    "        predicted: predicted values in 1D numpy array\n",
    "        return: float\n",
    "    \"\"\"\n",
    "    cost = (true - predicted)**2\n",
    "\n",
    "    # true above threshold (case 1)\n",
    "    mask = true > THRESHOLD\n",
    "    mask_w1 = np.logical_and(predicted>=true,mask)\n",
    "    mask_w2 = np.logical_and(np.logical_and(predicted<true,predicted >=THRESHOLD),mask)\n",
    "    mask_w3 = np.logical_and(predicted<THRESHOLD,mask)\n",
    "\n",
    "    cost[mask_w1] = cost[mask_w1]*W1\n",
    "    cost[mask_w2] = cost[mask_w2]*W2\n",
    "    cost[mask_w3] = cost[mask_w3]*W3\n",
    "\n",
    "    # true value below threshold (case 2)\n",
    "    mask = true <= THRESHOLD\n",
    "    mask_w1 = np.logical_and(predicted>true,mask)\n",
    "    mask_w2 = np.logical_and(predicted<=true,mask)\n",
    "\n",
    "    cost[mask_w1] = cost[mask_w1]*W1\n",
    "    cost[mask_w2] = cost[mask_w2]*W2\n",
    "\n",
    "    reward = W4*np.logical_and(predicted < THRESHOLD,true<THRESHOLD)\n",
    "    if reward is None:\n",
    "        reward = 0\n",
    "    return np.mean(cost) - np.mean(reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to set up the model we will need different object :\n",
    "* a Gaussian Process (GP) Model\n",
    "* a Likelihood\n",
    "* a Mean (defining the prior mean of the GP)\n",
    "* a Kernel (defining the prior covariance of the GP)\n",
    "* a MultivariateNormal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to construct the model we had to choose the Mean, the kernel and the multivariatenormal distribution. From the GPytorch package we choosed the following one :\n",
    "* Mean : ConstantMean() [https://docs.gpytorch.ai/en/v1.1.1/_modules/gpytorch/means/constant_mean.html#ConstantMean]\n",
    "* Kernel : RBFKernel() (squared exponential kernel)  [https://docs.gpytorch.ai/en/v1.1.1/_modules/gpytorch/kernels/]\n",
    "\n",
    "    $\\begin{equation*}\n",
    "       k_{\\text{RBF}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\exp \\left( -\\frac{1}{2}\n",
    "       (\\mathbf{x_1} - \\mathbf{x_2})^\\top \\Theta^{-2} (\\mathbf{x_1} - \\mathbf{x_2}) \\right)\n",
    "    \\end{equation*}$\n",
    "    \n",
    "\n",
    "* Distribution : MultivariateNormal(mean, covar) [https://docs.gpytorch.ai/en/v1.1.1/_modules/gpytorch/distributions/multivariate_normal.html#MultivariateNormal] . Recap: a vector X or random variable is said to have a multivariate normal distribution if its density function f(X) is of the form :\n",
    "\n",
    "    $\\begin{eqnarray}\n",
    "    f({\\bf X}) & = & f(X_1, \\, X_2, \\, \\ldots, \\, X_p) \\\\\n",
    "         & = & \\left( \\frac{1}{2 \\pi} \\right)^{p / 2} |{\\bf \\Sigma}|^{-1/2} \\mbox{exp} \n",
    "               \\left[ -\\frac{1}{2} ({\\bf X} - {\\bf m})' {\\bf \\Sigma}^{-1} ({\\bf X} - {\\bf m}) \\right] \\, ,\n",
    "    \\end{eqnarray}$\n",
    "\n",
    "\n",
    "Other possibilities of mean or kernel could have been used, but these ones well fited the task. You can find the different package here : [https://docs.gpytorch.ai/en/v1.1.1/index.html]\n",
    "\n",
    "A GPModel is composed as follow :\n",
    "\n",
    "* An __init__ method that takes the trraining data (X, y) and a likelihood and construct what is necessecary for the *forward* function of the model. It construct the mean and the covar (kernel) Modules\n",
    "* A __forward__ methos that takes as argument *n x d* [X]  data and returns a MultivariateNormal with the prior mean and covariance evaluated at X. So it concretely return mu(X) and the *n x n* covariance matrix Kxx.\n",
    "\n",
    "In GPyTorch, an ExactGP has .train() and .eval() mode that are used respectively to optimize model hyperparameters and to compute prediction.\n",
    "\n",
    "NB: We tried the RQKernel() (rational quadratic kernel) but result were not good enough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplest form of Gaussian Model, Inference (cf. Tutorial)\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model and likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said in the project description, with Bayesian models, a commonly used principle in choosing the right kernel or hyper-parameters is to use the \"data likelihood\", otherwise known as the marginal likelihood to find the best model. We decided to follow this advice and to use the Gaussian Likelihood proposed by GPyTorch (and we will see later that we use the ExactMarginalLogLikelihood as losse function)\n",
    "\n",
    "Moreover, as we use GPyTorch function we have ton use tensor instead of numpy array. This is why we 'cast' our training set into Tensor.\n",
    "\n",
    "We already define the number of iteration use for the training loop. Be default this number is 50, we try different value (25, 100, 150) but 50 seemed to us being the best compromise between performance and computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we need to convert our training set into tensor\n",
    "train_x = torch.Tensor(train_x)\n",
    "train_y = torch.Tensor(train_y)\n",
    "\n",
    "#Then we initialize the model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)\n",
    "\n",
    "# Number of iteration for the training loop\n",
    "iteration = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large scale learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is stated in the project description that : \n",
    "\n",
    "> Natively, GP inference is computationally intensive for large datasets and common-place computers. The inference requires O(N^3) basic operations in order find the posterior distributions. For large datasets this becomes infeasible. In order to solve this problem, practitioners use forms of low-rank approximations. The most popular are the NystrÃ¶m method, using random features and/or other scalable clustering-based approaches\n",
    "\n",
    "Fortunately, with GPyTorch the computation time was quite reasonable (around 2 min), so we decided to not implement this method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit/Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to fit our model, train the hyperparameters etc. As described in the tutorial :\n",
    "> The most obvious difference here compared to many other GP implementations is that, as in standard PyTorch, the core training loop is written by the user. In GPyTorch, we make use of the standard PyTorch optimizers as from torch.optim, and all trainable parameters of the model should be of type torch.nn.Parameter. Because GP models directly extend torch.nn.Module, calls to methods like model.parameters() or model.named_parameters() function as you might expect coming from PyTorch.\n",
    "\n",
    "In order to well fiting our model we will have to : \n",
    "* Train our model \n",
    "* Find an adequat optimizer \n",
    "* Compute the marginal log likelihood (MLL)\n",
    "* Define a training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we tell our model to turn the train mode. We can do it via the pytorch function __.train()__ . This function is define as follow :\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        r\"\"\"Sets the module in training mode.\"\"\"      \n",
    "        self.training = mode\n",
    "        for module in self.children():\n",
    "            module.train(mode)\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianLikelihood(\n",
       "  (noise_covar): HomoskedasticNoise(\n",
       "    (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we search a good optimizer. We tried the two following one :\n",
    "* Adaptive Moment Estimation (Adam) optimizer with learning rate 0.01 ?\n",
    "* SGD optimizer with learnin rate 0.01 and momentum 0.9\n",
    "\n",
    "You can find more info on both optimizer here [https://pytorch.org/docs/stable/optim.html]\n",
    "\n",
    "The adam optimizer can be described as follow :\n",
    "* Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models.\n",
    "* Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n",
    "* Adam is relatively easy to configure where the default configuration parameters do well on most problems.\n",
    "* We compute the decaying averages of past and past squared gradients __mt__ and __vt__ respectively as follows: \n",
    "$\n",
    "    \\begin{align} \n",
    "    \\begin{split} \n",
    "        m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ \n",
    "        v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \n",
    "    \\end{split} \n",
    "    \\end{align} \n",
    "$\n",
    "\n",
    "More info on the different optimizer that exist and how they work her : [https://ruder.io/optimizing-gradient-descent/index.html]\n",
    "\n",
    "In PyTorch, the adam optimizer is presented as follow : torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "\n",
    "After testing we choosed the Adam optimizer as it worked better with our model. We also tried different learning rate value (0.001, 0.0001 etc) but 0.01 was gave us the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal Log Likelihood "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to compute, or approximate/bound, the MLL.\n",
    "$$\n",
    "\\begin{equation*}\n",
    "   \\mathcal{L} = p_f(\\mathbf y \\! \\mid \\! \\mathbf X)\n",
    "   = \\int p \\left( \\mathbf y \\! \\mid \\! f(\\mathbf X) \\right) \\: p(f(\\mathbf X) \\! \\mid \\! \\mathbf X) \\: d f\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "In GPyTorch there is the exact marginal log likelihood for an exact Gaussian process with a Gaussian Likelihood : __gpytorch.mlls.ExactMarginalLogLikelihood__ \n",
    "\n",
    "Other possibilities as approximatinf the MML using variational evidence lower bound (ELBO)  could have been use but it was not necessary in our case as the ExactMarginalLogLikelihood was exactly what we needed.\n",
    "\n",
    "More details here : [ https://docs.gpytorch.ai/en/v1.1.1/marginal_log_likelihoods.html ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to define a training loop. A basic training loop works as follow :\n",
    "\n",
    "1. Zero all parameters graddients\n",
    "2. Call the model and compute the loss\n",
    "3. Call backward on the loss to fill in gradients\n",
    "4. Take a step on the optimizer\n",
    "\n",
    "Note that we compute the loss here using the MLL. We also tried to compute it using the cost function given in the project description :\n",
    "\n",
    "$ \\mathcal{Loss} = \\frac{1}{n} \\sum_{i=1}^n \\mathcal{L}_C (f(x_i),\\hat{f}(x_i)) - 0.04 \\times \\frac{1}{n} \\sum_{i=1}^n 1(f(x_i)<\\theta \\text{ and } \\hat{f}(x_i)<\\theta  ) $\n",
    "\n",
    "with : \n",
    "\n",
    "$ \\mathcal{L}_C(f(x), \\hat{f}(x)) =\n",
    "    \\begin{cases}\n",
    "        1 \\times (f(x) - \\hat{f}(x))^2  \\enspace & \\text{if } f(x) > \\theta, \\hat{f}(x) \\ge f(x) \\text{ or } f(x) \\le \\theta, \\hat{f}(x) > f(x), \\\\\n",
    "        20 \\times (f(x) - \\hat{f}(x))^2  & \\text{if } f(x) > \\theta, f(x) > \\hat{f}(x) \\ge \\theta \\text{ or } f(x) \\le \\theta, \\hat{f}(x) \\le f(x), \\\\\n",
    "        100 \\times (f(x) - \\hat{f}(x))^2 & \\text{if } f(x) > \\theta, \\hat{f}(x) < \\theta.\n",
    "    \\end{cases} $\n",
    "\n",
    "After adapting the method given in the tempalte such that it works with Tensor instead of numpy array, we find out that it was not performant enough compared to the MLL. Then we decided to keep the MLL function as our loss function.\n",
    "\n",
    "Following this schema we obtained the following loop :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/200 - Loss: 0.741   lengthscale: 0.693   noise: 0.693\n",
      "Iter 2/200 - Loss: 0.737   lengthscale: 0.698   noise: 0.688\n",
      "Iter 3/200 - Loss: 0.734   lengthscale: 0.703   noise: 0.683\n",
      "Iter 4/200 - Loss: 0.730   lengthscale: 0.708   noise: 0.678\n",
      "Iter 5/200 - Loss: 0.726   lengthscale: 0.713   noise: 0.673\n",
      "Iter 6/200 - Loss: 0.723   lengthscale: 0.718   noise: 0.669\n",
      "Iter 7/200 - Loss: 0.719   lengthscale: 0.724   noise: 0.664\n",
      "Iter 8/200 - Loss: 0.716   lengthscale: 0.729   noise: 0.659\n",
      "Iter 9/200 - Loss: 0.712   lengthscale: 0.734   noise: 0.654\n",
      "Iter 10/200 - Loss: 0.708   lengthscale: 0.739   noise: 0.649\n",
      "Iter 11/200 - Loss: 0.705   lengthscale: 0.744   noise: 0.644\n",
      "Iter 12/200 - Loss: 0.701   lengthscale: 0.749   noise: 0.640\n",
      "Iter 13/200 - Loss: 0.697   lengthscale: 0.754   noise: 0.635\n",
      "Iter 14/200 - Loss: 0.693   lengthscale: 0.760   noise: 0.630\n",
      "Iter 15/200 - Loss: 0.690   lengthscale: 0.765   noise: 0.626\n",
      "Iter 16/200 - Loss: 0.686   lengthscale: 0.770   noise: 0.621\n",
      "Iter 17/200 - Loss: 0.682   lengthscale: 0.775   noise: 0.616\n",
      "Iter 18/200 - Loss: 0.679   lengthscale: 0.781   noise: 0.612\n",
      "Iter 19/200 - Loss: 0.675   lengthscale: 0.786   noise: 0.607\n",
      "Iter 20/200 - Loss: 0.671   lengthscale: 0.791   noise: 0.603\n",
      "Iter 21/200 - Loss: 0.667   lengthscale: 0.796   noise: 0.598\n",
      "Iter 22/200 - Loss: 0.664   lengthscale: 0.802   noise: 0.593\n",
      "Iter 23/200 - Loss: 0.660   lengthscale: 0.807   noise: 0.589\n",
      "Iter 24/200 - Loss: 0.656   lengthscale: 0.812   noise: 0.585\n",
      "Iter 25/200 - Loss: 0.652   lengthscale: 0.817   noise: 0.580\n",
      "Iter 26/200 - Loss: 0.648   lengthscale: 0.823   noise: 0.576\n",
      "Iter 27/200 - Loss: 0.645   lengthscale: 0.828   noise: 0.571\n",
      "Iter 28/200 - Loss: 0.641   lengthscale: 0.833   noise: 0.567\n",
      "Iter 29/200 - Loss: 0.637   lengthscale: 0.838   noise: 0.562\n",
      "Iter 30/200 - Loss: 0.633   lengthscale: 0.843   noise: 0.558\n",
      "Iter 31/200 - Loss: 0.629   lengthscale: 0.848   noise: 0.554\n",
      "Iter 32/200 - Loss: 0.625   lengthscale: 0.853   noise: 0.549\n",
      "Iter 33/200 - Loss: 0.621   lengthscale: 0.858   noise: 0.545\n",
      "Iter 34/200 - Loss: 0.617   lengthscale: 0.863   noise: 0.541\n",
      "Iter 35/200 - Loss: 0.614   lengthscale: 0.867   noise: 0.537\n",
      "Iter 36/200 - Loss: 0.610   lengthscale: 0.872   noise: 0.533\n",
      "Iter 37/200 - Loss: 0.606   lengthscale: 0.876   noise: 0.528\n",
      "Iter 38/200 - Loss: 0.602   lengthscale: 0.881   noise: 0.524\n",
      "Iter 39/200 - Loss: 0.598   lengthscale: 0.885   noise: 0.520\n",
      "Iter 40/200 - Loss: 0.594   lengthscale: 0.889   noise: 0.516\n",
      "Iter 41/200 - Loss: 0.590   lengthscale: 0.893   noise: 0.512\n",
      "Iter 42/200 - Loss: 0.586   lengthscale: 0.898   noise: 0.508\n",
      "Iter 43/200 - Loss: 0.582   lengthscale: 0.902   noise: 0.504\n",
      "Iter 44/200 - Loss: 0.578   lengthscale: 0.905   noise: 0.500\n",
      "Iter 45/200 - Loss: 0.574   lengthscale: 0.909   noise: 0.496\n",
      "Iter 46/200 - Loss: 0.570   lengthscale: 0.912   noise: 0.492\n",
      "Iter 47/200 - Loss: 0.566   lengthscale: 0.916   noise: 0.488\n",
      "Iter 48/200 - Loss: 0.562   lengthscale: 0.918   noise: 0.484\n",
      "Iter 49/200 - Loss: 0.558   lengthscale: 0.921   noise: 0.480\n",
      "Iter 50/200 - Loss: 0.554   lengthscale: 0.924   noise: 0.476\n",
      "Iter 51/200 - Loss: 0.550   lengthscale: 0.926   noise: 0.472\n",
      "Iter 52/200 - Loss: 0.546   lengthscale: 0.928   noise: 0.468\n",
      "Iter 53/200 - Loss: 0.542   lengthscale: 0.930   noise: 0.464\n",
      "Iter 54/200 - Loss: 0.538   lengthscale: 0.931   noise: 0.460\n",
      "Iter 55/200 - Loss: 0.533   lengthscale: 0.933   noise: 0.457\n",
      "Iter 56/200 - Loss: 0.529   lengthscale: 0.934   noise: 0.453\n",
      "Iter 57/200 - Loss: 0.525   lengthscale: 0.935   noise: 0.449\n",
      "Iter 58/200 - Loss: 0.521   lengthscale: 0.936   noise: 0.445\n",
      "Iter 59/200 - Loss: 0.517   lengthscale: 0.937   noise: 0.442\n",
      "Iter 60/200 - Loss: 0.513   lengthscale: 0.938   noise: 0.438\n",
      "Iter 61/200 - Loss: 0.509   lengthscale: 0.938   noise: 0.434\n",
      "Iter 62/200 - Loss: 0.504   lengthscale: 0.939   noise: 0.431\n",
      "Iter 63/200 - Loss: 0.500   lengthscale: 0.940   noise: 0.427\n",
      "Iter 64/200 - Loss: 0.496   lengthscale: 0.940   noise: 0.423\n",
      "Iter 65/200 - Loss: 0.492   lengthscale: 0.941   noise: 0.420\n",
      "Iter 66/200 - Loss: 0.488   lengthscale: 0.942   noise: 0.416\n",
      "Iter 67/200 - Loss: 0.483   lengthscale: 0.942   noise: 0.413\n",
      "Iter 68/200 - Loss: 0.479   lengthscale: 0.942   noise: 0.409\n",
      "Iter 69/200 - Loss: 0.475   lengthscale: 0.942   noise: 0.406\n",
      "Iter 70/200 - Loss: 0.471   lengthscale: 0.941   noise: 0.402\n",
      "Iter 71/200 - Loss: 0.466   lengthscale: 0.941   noise: 0.399\n",
      "Iter 72/200 - Loss: 0.462   lengthscale: 0.940   noise: 0.395\n",
      "Iter 73/200 - Loss: 0.458   lengthscale: 0.939   noise: 0.392\n",
      "Iter 74/200 - Loss: 0.454   lengthscale: 0.938   noise: 0.389\n",
      "Iter 75/200 - Loss: 0.449   lengthscale: 0.937   noise: 0.385\n",
      "Iter 76/200 - Loss: 0.445   lengthscale: 0.936   noise: 0.382\n",
      "Iter 77/200 - Loss: 0.441   lengthscale: 0.934   noise: 0.379\n",
      "Iter 78/200 - Loss: 0.436   lengthscale: 0.933   noise: 0.375\n",
      "Iter 79/200 - Loss: 0.432   lengthscale: 0.931   noise: 0.372\n",
      "Iter 80/200 - Loss: 0.428   lengthscale: 0.930   noise: 0.369\n",
      "Iter 81/200 - Loss: 0.423   lengthscale: 0.928   noise: 0.366\n",
      "Iter 82/200 - Loss: 0.419   lengthscale: 0.926   noise: 0.362\n",
      "Iter 83/200 - Loss: 0.415   lengthscale: 0.924   noise: 0.359\n",
      "Iter 84/200 - Loss: 0.410   lengthscale: 0.923   noise: 0.356\n",
      "Iter 85/200 - Loss: 0.406   lengthscale: 0.921   noise: 0.353\n",
      "Iter 86/200 - Loss: 0.401   lengthscale: 0.919   noise: 0.350\n",
      "Iter 87/200 - Loss: 0.397   lengthscale: 0.918   noise: 0.347\n",
      "Iter 88/200 - Loss: 0.393   lengthscale: 0.916   noise: 0.343\n",
      "Iter 89/200 - Loss: 0.388   lengthscale: 0.915   noise: 0.340\n",
      "Iter 90/200 - Loss: 0.384   lengthscale: 0.914   noise: 0.337\n",
      "Iter 91/200 - Loss: 0.379   lengthscale: 0.913   noise: 0.334\n",
      "Iter 92/200 - Loss: 0.375   lengthscale: 0.912   noise: 0.331\n",
      "Iter 93/200 - Loss: 0.371   lengthscale: 0.911   noise: 0.328\n",
      "Iter 94/200 - Loss: 0.366   lengthscale: 0.910   noise: 0.325\n",
      "Iter 95/200 - Loss: 0.362   lengthscale: 0.910   noise: 0.323\n",
      "Iter 96/200 - Loss: 0.357   lengthscale: 0.909   noise: 0.320\n",
      "Iter 97/200 - Loss: 0.353   lengthscale: 0.909   noise: 0.317\n",
      "Iter 98/200 - Loss: 0.348   lengthscale: 0.908   noise: 0.314\n",
      "Iter 99/200 - Loss: 0.344   lengthscale: 0.908   noise: 0.311\n",
      "Iter 100/200 - Loss: 0.339   lengthscale: 0.907   noise: 0.308\n",
      "Iter 101/200 - Loss: 0.335   lengthscale: 0.907   noise: 0.305\n",
      "Iter 102/200 - Loss: 0.330   lengthscale: 0.906   noise: 0.303\n",
      "Iter 103/200 - Loss: 0.326   lengthscale: 0.905   noise: 0.300\n",
      "Iter 104/200 - Loss: 0.321   lengthscale: 0.903   noise: 0.297\n",
      "Iter 105/200 - Loss: 0.317   lengthscale: 0.902   noise: 0.294\n",
      "Iter 106/200 - Loss: 0.312   lengthscale: 0.901   noise: 0.292\n",
      "Iter 107/200 - Loss: 0.307   lengthscale: 0.900   noise: 0.289\n",
      "Iter 108/200 - Loss: 0.303   lengthscale: 0.898   noise: 0.286\n",
      "Iter 109/200 - Loss: 0.298   lengthscale: 0.897   noise: 0.284\n",
      "Iter 110/200 - Loss: 0.294   lengthscale: 0.896   noise: 0.281\n",
      "Iter 111/200 - Loss: 0.289   lengthscale: 0.894   noise: 0.278\n",
      "Iter 112/200 - Loss: 0.285   lengthscale: 0.892   noise: 0.276\n",
      "Iter 113/200 - Loss: 0.280   lengthscale: 0.891   noise: 0.273\n",
      "Iter 114/200 - Loss: 0.275   lengthscale: 0.889   noise: 0.271\n",
      "Iter 115/200 - Loss: 0.271   lengthscale: 0.887   noise: 0.268\n",
      "Iter 116/200 - Loss: 0.266   lengthscale: 0.885   noise: 0.266\n",
      "Iter 117/200 - Loss: 0.262   lengthscale: 0.884   noise: 0.263\n",
      "Iter 118/200 - Loss: 0.257   lengthscale: 0.883   noise: 0.261\n",
      "Iter 119/200 - Loss: 0.252   lengthscale: 0.882   noise: 0.258\n",
      "Iter 120/200 - Loss: 0.248   lengthscale: 0.881   noise: 0.256\n",
      "Iter 121/200 - Loss: 0.243   lengthscale: 0.880   noise: 0.253\n",
      "Iter 122/200 - Loss: 0.238   lengthscale: 0.879   noise: 0.251\n",
      "Iter 123/200 - Loss: 0.234   lengthscale: 0.878   noise: 0.249\n",
      "Iter 124/200 - Loss: 0.229   lengthscale: 0.877   noise: 0.246\n",
      "Iter 125/200 - Loss: 0.224   lengthscale: 0.876   noise: 0.244\n",
      "Iter 126/200 - Loss: 0.220   lengthscale: 0.875   noise: 0.242\n",
      "Iter 127/200 - Loss: 0.215   lengthscale: 0.874   noise: 0.239\n",
      "Iter 128/200 - Loss: 0.210   lengthscale: 0.873   noise: 0.237\n",
      "Iter 129/200 - Loss: 0.206   lengthscale: 0.873   noise: 0.235\n",
      "Iter 130/200 - Loss: 0.201   lengthscale: 0.872   noise: 0.233\n",
      "Iter 131/200 - Loss: 0.196   lengthscale: 0.871   noise: 0.230\n",
      "Iter 132/200 - Loss: 0.191   lengthscale: 0.870   noise: 0.228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 133/200 - Loss: 0.187   lengthscale: 0.870   noise: 0.226\n",
      "Iter 134/200 - Loss: 0.182   lengthscale: 0.868   noise: 0.224\n",
      "Iter 135/200 - Loss: 0.177   lengthscale: 0.867   noise: 0.222\n",
      "Iter 136/200 - Loss: 0.173   lengthscale: 0.867   noise: 0.220\n",
      "Iter 137/200 - Loss: 0.168   lengthscale: 0.865   noise: 0.217\n",
      "Iter 138/200 - Loss: 0.163   lengthscale: 0.864   noise: 0.215\n",
      "Iter 139/200 - Loss: 0.158   lengthscale: 0.863   noise: 0.213\n",
      "Iter 140/200 - Loss: 0.154   lengthscale: 0.862   noise: 0.211\n",
      "Iter 141/200 - Loss: 0.149   lengthscale: 0.861   noise: 0.209\n",
      "Iter 142/200 - Loss: 0.144   lengthscale: 0.859   noise: 0.207\n",
      "Iter 143/200 - Loss: 0.139   lengthscale: 0.857   noise: 0.205\n",
      "Iter 144/200 - Loss: 0.135   lengthscale: 0.855   noise: 0.203\n",
      "Iter 145/200 - Loss: 0.130   lengthscale: 0.853   noise: 0.201\n",
      "Iter 146/200 - Loss: 0.125   lengthscale: 0.851   noise: 0.199\n",
      "Iter 147/200 - Loss: 0.120   lengthscale: 0.848   noise: 0.197\n",
      "Iter 148/200 - Loss: 0.115   lengthscale: 0.846   noise: 0.195\n",
      "Iter 149/200 - Loss: 0.111   lengthscale: 0.843   noise: 0.193\n",
      "Iter 150/200 - Loss: 0.106   lengthscale: 0.840   noise: 0.192\n",
      "Iter 151/200 - Loss: 0.101   lengthscale: 0.837   noise: 0.190\n",
      "Iter 152/200 - Loss: 0.096   lengthscale: 0.835   noise: 0.188\n",
      "Iter 153/200 - Loss: 0.091   lengthscale: 0.833   noise: 0.186\n",
      "Iter 154/200 - Loss: 0.087   lengthscale: 0.831   noise: 0.184\n",
      "Iter 155/200 - Loss: 0.082   lengthscale: 0.829   noise: 0.182\n",
      "Iter 156/200 - Loss: 0.077   lengthscale: 0.827   noise: 0.181\n",
      "Iter 157/200 - Loss: 0.072   lengthscale: 0.825   noise: 0.179\n",
      "Iter 158/200 - Loss: 0.067   lengthscale: 0.823   noise: 0.177\n",
      "Iter 159/200 - Loss: 0.063   lengthscale: 0.820   noise: 0.175\n",
      "Iter 160/200 - Loss: 0.058   lengthscale: 0.817   noise: 0.174\n",
      "Iter 161/200 - Loss: 0.053   lengthscale: 0.814   noise: 0.172\n",
      "Iter 162/200 - Loss: 0.048   lengthscale: 0.811   noise: 0.170\n",
      "Iter 163/200 - Loss: 0.043   lengthscale: 0.809   noise: 0.169\n",
      "Iter 164/200 - Loss: 0.038   lengthscale: 0.806   noise: 0.167\n",
      "Iter 165/200 - Loss: 0.034   lengthscale: 0.803   noise: 0.165\n",
      "Iter 166/200 - Loss: 0.029   lengthscale: 0.801   noise: 0.164\n",
      "Iter 167/200 - Loss: 0.024   lengthscale: 0.797   noise: 0.162\n",
      "Iter 168/200 - Loss: 0.019   lengthscale: 0.794   noise: 0.160\n",
      "Iter 169/200 - Loss: 0.014   lengthscale: 0.790   noise: 0.159\n",
      "Iter 170/200 - Loss: 0.009   lengthscale: 0.786   noise: 0.157\n",
      "Iter 171/200 - Loss: 0.004   lengthscale: 0.782   noise: 0.156\n",
      "Iter 172/200 - Loss: -0.000   lengthscale: 0.777   noise: 0.154\n",
      "Iter 173/200 - Loss: -0.005   lengthscale: 0.772   noise: 0.153\n",
      "Iter 174/200 - Loss: -0.010   lengthscale: 0.767   noise: 0.151\n",
      "Iter 175/200 - Loss: -0.015   lengthscale: 0.762   noise: 0.149\n",
      "Iter 176/200 - Loss: -0.020   lengthscale: 0.756   noise: 0.148\n",
      "Iter 177/200 - Loss: -0.025   lengthscale: 0.750   noise: 0.147\n",
      "Iter 178/200 - Loss: -0.030   lengthscale: 0.743   noise: 0.145\n",
      "Iter 179/200 - Loss: -0.035   lengthscale: 0.736   noise: 0.144\n",
      "Iter 180/200 - Loss: -0.040   lengthscale: 0.728   noise: 0.142\n",
      "Iter 181/200 - Loss: -0.045   lengthscale: 0.720   noise: 0.141\n",
      "Iter 182/200 - Loss: -0.049   lengthscale: 0.711   noise: 0.139\n",
      "Iter 183/200 - Loss: -0.054   lengthscale: 0.701   noise: 0.138\n",
      "Iter 184/200 - Loss: -0.059   lengthscale: 0.691   noise: 0.137\n",
      "Iter 185/200 - Loss: -0.064   lengthscale: 0.681   noise: 0.135\n",
      "Iter 186/200 - Loss: -0.069   lengthscale: 0.669   noise: 0.134\n",
      "Iter 187/200 - Loss: -0.074   lengthscale: 0.658   noise: 0.132\n",
      "Iter 188/200 - Loss: -0.079   lengthscale: 0.646   noise: 0.131\n",
      "Iter 189/200 - Loss: -0.084   lengthscale: 0.634   noise: 0.130\n",
      "Iter 190/200 - Loss: -0.089   lengthscale: 0.622   noise: 0.129\n",
      "Iter 191/200 - Loss: -0.094   lengthscale: 0.610   noise: 0.127\n",
      "Iter 192/200 - Loss: -0.099   lengthscale: 0.598   noise: 0.126\n",
      "Iter 193/200 - Loss: -0.104   lengthscale: 0.585   noise: 0.125\n",
      "Iter 194/200 - Loss: -0.109   lengthscale: 0.573   noise: 0.123\n",
      "Iter 195/200 - Loss: -0.114   lengthscale: 0.561   noise: 0.122\n",
      "Iter 196/200 - Loss: -0.119   lengthscale: 0.550   noise: 0.121\n",
      "Iter 197/200 - Loss: -0.124   lengthscale: 0.538   noise: 0.120\n",
      "Iter 198/200 - Loss: -0.129   lengthscale: 0.527   noise: 0.119\n",
      "Iter 199/200 - Loss: -0.134   lengthscale: 0.516   noise: 0.117\n",
      "Iter 200/200 - Loss: -0.139   lengthscale: 0.505   noise: 0.116\n"
     ]
    }
   ],
   "source": [
    "# Create a table to keep the loss in order to plot them after\n",
    "loss_table = []\n",
    "\n",
    "# The training loop :\n",
    "for i in range(iteration):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    \n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Print state\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, iteration, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    \n",
    "    loss_table.append(loss.item())\n",
    "    \n",
    "    # Take a step on the otimizer\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we have to remember that we defined the GP model such that it returns MultivariateNormal containing the posterior mean and covariance. Then we need to 'treat' the results of our GP model prediction in order to obtain the final resutls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, as for the training data, we need to convert our testing set into tensor : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.Tensor(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to turn our model into the eval mode (until now it was in training mode) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the predicition (in form of MultivariateNormal) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these, we can obtain the 'precise' prediction by taking the mean of each normal distribution. Warning: We need to apply detach() and numpy() because results were given in torch format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = y_obs.mean.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment we thought we were good, but not good enought for the baseline apparently. So we hade to improve our final results. We already tried to change the optimizer, the kernel, the loss function used etc. But nothing was good enough to outperform the baseline. Then we saw in the task description that :\n",
    "\n",
    "> We utilize a specifically designed cost function, where deviation from the true concentration levels is penalized, and you are rewarded for correctly predicting safe regions. Under this specific cost function, the mean prediction might not be optimal. Note that the mean prediction refers to the optimal decision with respect to a general squared loss and some posterior distribution over the true value to be predicted.\n",
    "\n",
    "From this we knew that we had to limit our errors that cost a lot into the losse function. As the threshold for a 'safe place' is 0.5, we decided to remove predicted value bellow 0.5 where we are not sure enought of the quality of the prediction. This is how we did it : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we get the variance of each prediction using the same method as for the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_obs = y_obs.variance.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define a normal continuous random variable for each predictions with the predicted variance and mean, using the function __norm()__ from scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_ditribution = norm(y_preds, variance_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for all normal continous random variable we can compute de CDF at 0.5 (eq. the Threshold), knowing that the y_real are between 0 and 1 by definitnon, and that the THRESHOLD use in the loss function for safe places is 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_half = norm_ditribution.cdf([0.5] * len(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we now use these cdf results in order to see where errors could have occured. We want to replace every prediction that is less than 0.5 (prediction that could cost us a lot of penalities) and where the cdf at 0.5 (Probability that the prediction is in fact less than 0.5) for this prediction is less than our 'confidence value'. But we still need to determine our 'confidence value'. To do it we tried different values between 0.6 and 0.9 and we see that a confidence of 0.7 give us the best results. We replace thes values by 5 + epsilon such that the loss function will not considere them as 'safe place' and then not penalize us for classifying those regions as safe when they are probbaly not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 0.7\n",
    "epsilon = 0.00000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds[(y_preds < 0.5) & (cdf_half < confidence)] = 0.5 + epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running this code with the Docker (sudo bash runner.sh) we obtained 0.033 as result. The PUBLIC baseline is 0.064. We decided to upload our code and results to the submit system in order to see if we can also outperformed the PRIVATE baseline (0.069). We obtained a score of 0.048, which is better than the PRIVATE baseline. Even if we saw that a lot of people had better scores, we decided to stop here the research as we already spent a lot of time on the task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib:$HOME=/home/atroska\n",
      "DEBUG:matplotlib:CONFIGDIR=/home/atroska/.config/matplotlib\n",
      "DEBUG:matplotlib:matplotlib data path: /usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data\n",
      "DEBUG:matplotlib:loaded rc file /usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data/matplotlibrc\n",
      "DEBUG:matplotlib:matplotlib version 3.0.2\n",
      "DEBUG:matplotlib:interactive is False\n",
      "DEBUG:matplotlib:platform is linux\n",
      "DEBUG:matplotlib:loaded modules: ['builtins', 'sys', '_frozen_importlib', '_imp', '_warnings', '_thread', '_weakref', '_frozen_importlib_external', '_io', 'marshal', 'posix', 'zipimport', 'encodings', 'codecs', '_codecs', 'encodings.aliases', 'encodings.utf_8', '_signal', '__main__', 'encodings.latin_1', 'io', 'abc', '_weakrefset', '_bootlocale', '_locale', 'site', 'os', 'errno', 'stat', '_stat', 'posixpath', 'genericpath', 'os.path', '_collections_abc', '_sitebuiltins', 'sysconfig', '_sysconfigdata_m_linux_x86_64-linux-gnu', 'types', 'functools', '_functools', 'collections', 'operator', '_operator', 'keyword', 'heapq', '_heapq', 'itertools', 'reprlib', '_collections', 'weakref', 'collections.abc', 'importlib', 'importlib._bootstrap', 'importlib._bootstrap_external', 'warnings', 'importlib.util', 'importlib.abc', 'importlib.machinery', 'contextlib', 'mpl_toolkits', 'zope', 'sitecustomize', 'apport_python_hook', 'runpy', 'pkgutil', 'ipykernel', 'ipykernel._version', 'ipykernel.connect', '__future__', 'json', 'json.decoder', 're', 'enum', 'sre_compile', '_sre', 'sre_parse', 'sre_constants', 'copyreg', 'json.scanner', '_json', 'json.encoder', 'subprocess', 'time', 'signal', '_posixsubprocess', 'select', 'selectors', 'math', 'threading', 'traceback', 'linecache', 'tokenize', 'token', 'IPython', 'IPython.core', 'IPython.core.getipython', 'IPython.core.release', 'IPython.core.application', 'atexit', 'copy', 'glob', 'fnmatch', 'logging', 'string', '_string', 'shutil', 'zlib', 'bz2', '_compression', '_bz2', 'lzma', '_lzma', 'pwd', 'grp', 'traitlets', 'traitlets.traitlets', 'inspect', 'ast', '_ast', 'dis', 'opcode', '_opcode', 'six', 'struct', '_struct', 'traitlets.utils', 'traitlets.utils.getargspec', 'traitlets.utils.importstring', 'ipython_genutils', 'ipython_genutils._version', 'ipython_genutils.py3compat', 'ipython_genutils.encoding', 'locale', 'platform', 'traitlets.utils.sentinel', 'traitlets.utils.bunch', 'traitlets._version', 'traitlets.config', 'traitlets.config.application', 'decorator', 'traitlets.config.configurable', 'traitlets.config.loader', 'argparse', 'textwrap', 'gettext', 'ipython_genutils.path', 'random', 'hashlib', '_hashlib', '_blake2', '_sha3', 'bisect', '_bisect', '_random', 'ipython_genutils.text', 'ipython_genutils.importstring', 'IPython.core.crashhandler', 'pprint', 'IPython.core.ultratb', 'pydoc', 'urllib', 'urllib.parse', 'IPython.core.debugger', 'bdb', 'IPython.utils', 'IPython.utils.PyColorize', 'IPython.utils.coloransi', 'IPython.utils.ipstruct', 'IPython.utils.colorable', 'pygments', 'pygments.util', 'IPython.utils.py3compat', 'IPython.utils.encoding', 'IPython.core.excolors', 'IPython.testing', 'IPython.testing.skipdoctest', 'pdb', 'cmd', 'code', 'codeop', 'IPython.core.display_trap', 'IPython.utils.path', 'IPython.utils.process', 'IPython.utils._process_posix', 'pexpect', 'pexpect.exceptions', 'pexpect.utils', 'pexpect.expect', 'pexpect.pty_spawn', 'pty', 'tty', 'termios', 'ptyprocess', 'ptyprocess.ptyprocess', 'fcntl', 'resource', 'ptyprocess.util', 'pexpect.spawnbase', 'pexpect.run', 'IPython.utils._process_common', 'shlex', 'IPython.utils.decorators', 'IPython.utils.data', 'IPython.utils.terminal', 'IPython.utils.sysinfo', 'IPython.utils._sysinfo', 'IPython.core.profiledir', 'IPython.paths', 'tempfile', 'IPython.utils.importstring', 'IPython.terminal', 'IPython.terminal.embed', 'IPython.core.compilerop', 'IPython.core.magic_arguments', 'IPython.core.error', 'IPython.utils.text', 'pathlib', 'ntpath', 'IPython.core.magic', 'getopt', 'IPython.core.oinspect', 'IPython.core.page', 'IPython.core.display', 'binascii', 'mimetypes', 'IPython.lib', 'IPython.lib.security', 'getpass', 'IPython.lib.pretty', 'datetime', '_datetime', 'IPython.utils.openpy', 'IPython.utils.dir2', 'IPython.utils.wildcard', 'pygments.lexers', 'pygments.lexers._mapping', 'pygments.modeline', 'pygments.plugin', 'pygments.lexers.python', 'pygments.lexer', 'pygments.filter', 'pygments.filters', 'pygments.token', 'pygments.regexopt', 'pygments.unistring', 'pygments.formatters', 'pygments.formatters._mapping', 'pygments.formatters.html', 'pygments.formatter', 'pygments.styles', 'IPython.core.inputtransformer2', 'typing', 'typing.io', 'typing.re', 'IPython.core.interactiveshell', 'asyncio', 'asyncio.base_events', 'concurrent', 'concurrent.futures', 'concurrent.futures._base', 'concurrent.futures.process', 'queue', 'multiprocessing', 'multiprocessing.context', 'multiprocessing.process', 'multiprocessing.reduction', 'pickle', '_compat_pickle', '_pickle', 'socket', '_socket', 'array', '__mp_main__', 'multiprocessing.connection', '_multiprocessing', 'multiprocessing.util', 'concurrent.futures.thread', 'asyncio.compat', 'asyncio.coroutines', 'asyncio.constants', 'asyncio.events', 'asyncio.base_futures', 'asyncio.log', 'asyncio.futures', 'asyncio.base_tasks', '_asyncio', 'asyncio.tasks', 'asyncio.locks', 'asyncio.protocols', 'asyncio.queues', 'asyncio.streams', 'asyncio.subprocess', 'asyncio.transports', 'asyncio.unix_events', 'asyncio.base_subprocess', 'asyncio.selector_events', 'ssl', 'ipaddress', '_ssl', 'base64', 'asyncio.sslproto', 'pickleshare', 'IPython.core.prefilter', 'IPython.core.autocall', 'IPython.core.macro', 'IPython.core.splitinput', 'IPython.core.alias', 'IPython.core.builtin_trap', 'IPython.core.events', 'backcall', 'backcall.backcall', 'IPython.core.displayhook', 'IPython.core.displaypub', 'IPython.core.extensions', 'IPython.core.formatters', 'IPython.utils.sentinel', 'IPython.core.history', 'sqlite3', 'sqlite3.dbapi2', '_sqlite3', 'IPython.core.logger', 'IPython.core.payload', 'IPython.core.usage', 'IPython.display', 'IPython.lib.display', 'html', 'html.entities', 'IPython.utils.io', 'IPython.utils.capture', 'IPython.utils.strdispatch', 'IPython.core.hooks', 'IPython.utils.syspathcontext', 'IPython.utils.tempdir', 'IPython.utils.contexts', 'IPython.core.async_helpers', 'IPython.terminal.interactiveshell', 'prompt_toolkit', 'prompt_toolkit.application', 'prompt_toolkit.application.application', 'prompt_toolkit.buffer', 'prompt_toolkit.application.current', 'prompt_toolkit.eventloop', 'prompt_toolkit.eventloop.base', 'prompt_toolkit.log', 'prompt_toolkit.eventloop.coroutine', 'prompt_toolkit.eventloop.defaults', 'prompt_toolkit.utils', 'six.moves', 'wcwidth', 'wcwidth.wcwidth', 'wcwidth.table_wide', 'wcwidth.table_zero', 'prompt_toolkit.cache', 'prompt_toolkit.eventloop.future', 'prompt_toolkit.eventloop.context', 'prompt_toolkit.eventloop.async_generator', 'six.moves.queue', 'prompt_toolkit.eventloop.event', 'prompt_toolkit.application.run_in_terminal', 'prompt_toolkit.auto_suggest', 'prompt_toolkit.filters', 'prompt_toolkit.filters.base', 'prompt_toolkit.filters.app', 'prompt_toolkit.enums', 'prompt_toolkit.filters.utils', 'prompt_toolkit.filters.cli', 'prompt_toolkit.clipboard', 'prompt_toolkit.clipboard.base', 'prompt_toolkit.selection', 'prompt_toolkit.clipboard.in_memory', 'prompt_toolkit.completion', 'prompt_toolkit.completion.base', 'prompt_toolkit.completion.filesystem', 'prompt_toolkit.completion.word_completer', 'prompt_toolkit.completion.fuzzy_completer', 'prompt_toolkit.document', 'prompt_toolkit.history', 'prompt_toolkit.search', 'prompt_toolkit.key_binding', 'prompt_toolkit.key_binding.key_bindings', 'prompt_toolkit.keys', 'prompt_toolkit.key_binding.vi_state', 'prompt_toolkit.validation', 'prompt_toolkit.input', 'prompt_toolkit.input.base', 'prompt_toolkit.input.defaults', 'prompt_toolkit.input.typeahead', 'prompt_toolkit.key_binding.bindings', 'prompt_toolkit.key_binding.bindings.page_navigation', 'prompt_toolkit.key_binding.bindings.scroll', 'prompt_toolkit.key_binding.defaults', 'prompt_toolkit.key_binding.bindings.basic', 'prompt_toolkit.key_binding.key_processor', 'prompt_toolkit.key_binding.bindings.named_commands', 'prompt_toolkit.key_binding.bindings.completion', 'prompt_toolkit.key_binding.bindings.emacs', 'prompt_toolkit.key_binding.bindings.vi', 'prompt_toolkit.input.vt100_parser', 'prompt_toolkit.input.ansi_escape_sequences', 'prompt_toolkit.key_binding.digraphs', 'prompt_toolkit.key_binding.bindings.mouse', 'prompt_toolkit.layout', 'prompt_toolkit.layout.containers', 'prompt_toolkit.layout.controls', 'prompt_toolkit.formatted_text', 'prompt_toolkit.formatted_text.base', 'prompt_toolkit.formatted_text.html', 'xml', 'xml.dom', 'xml.dom.domreg', 'xml.dom.minidom', 'xml.dom.minicompat', 'xml.dom.xmlbuilder', 'xml.dom.NodeFilter', 'prompt_toolkit.formatted_text.ansi', 'prompt_toolkit.output', 'prompt_toolkit.output.base', 'prompt_toolkit.layout.screen', 'prompt_toolkit.output.defaults', 'prompt_toolkit.output.color_depth', 'prompt_toolkit.output.vt100', 'prompt_toolkit.styles', 'prompt_toolkit.styles.base', 'prompt_toolkit.styles.defaults', 'prompt_toolkit.styles.style', 'prompt_toolkit.styles.named_colors', 'prompt_toolkit.styles.pygments', 'prompt_toolkit.styles.style_transformation', 'colorsys', 'prompt_toolkit.formatted_text.pygments', 'prompt_toolkit.formatted_text.utils', 'prompt_toolkit.lexers', 'prompt_toolkit.lexers.base', 'prompt_toolkit.lexers.pygments', 'prompt_toolkit.mouse_events', 'prompt_toolkit.layout.processors', 'prompt_toolkit.layout.utils', 'prompt_toolkit.layout.dimension', 'prompt_toolkit.layout.margins', 'prompt_toolkit.layout.layout', 'prompt_toolkit.layout.menus', 'prompt_toolkit.renderer', 'prompt_toolkit.layout.mouse_handlers', 'prompt_toolkit.key_binding.bindings.cpr', 'prompt_toolkit.key_binding.emacs_state', 'prompt_toolkit.layout.dummy', 'prompt_toolkit.application.dummy', 'prompt_toolkit.shortcuts', 'prompt_toolkit.shortcuts.dialogs', 'prompt_toolkit.key_binding.bindings.focus', 'prompt_toolkit.widgets', 'prompt_toolkit.widgets.base', 'prompt_toolkit.widgets.toolbars', 'prompt_toolkit.widgets.dialogs', 'prompt_toolkit.widgets.menus', 'prompt_toolkit.shortcuts.prompt', 'prompt_toolkit.key_binding.bindings.auto_suggest', 'prompt_toolkit.key_binding.bindings.open_in_editor', 'prompt_toolkit.shortcuts.utils', 'prompt_toolkit.shortcuts.progress_bar', 'prompt_toolkit.shortcuts.progress_bar.base', 'prompt_toolkit.shortcuts.progress_bar.formatters', 'prompt_toolkit.patch_stdout', 'pygments.style', 'IPython.terminal.debugger', 'IPython.core.completer', 'unicodedata', 'IPython.core.latex_symbols', 'IPython.utils.generics', 'jedi', 'jedi.api', 'parso', 'parso.parser', 'parso.tree', 'parso._compatibility', 'parso.utils', 'parso.pgen2', 'parso.pgen2.generator', 'parso.pgen2.grammar_parser', 'parso.python', 'parso.python.tokenize', 'parso.python.token', 'parso.grammar', 'parso.python.diff', 'difflib', 'parso.python.parser', 'parso.python.tree', 'parso.python.prefix', 'parso.cache', 'gc', 'parso.python.errors', 'parso.normalizer', 'parso.python.pep8', 'jedi._compatibility', 'jedi.parser_utils', 'jedi.debug', 'jedi.settings', 'jedi.cache', 'jedi.api.classes', 'jedi.evaluate', 'jedi.evaluate.utils', 'jedi.evaluate.imports', 'jedi.evaluate.sys_path', 'jedi.evaluate.cache', 'jedi.evaluate.base_context', 'jedi.common', 'jedi.common.context', 'jedi.evaluate.helpers', 'jedi.common.utils', 'jedi.evaluate.compiled', 'jedi.evaluate.compiled.context', 'jedi.evaluate.filters', 'jedi.evaluate.flow_analysis', 'jedi.evaluate.recursion', 'jedi.evaluate.lazy_context', 'jedi.evaluate.compiled.access', 'jedi.evaluate.compiled.getattr_static', 'jedi.evaluate.compiled.fake', 'jedi.evaluate.analysis', 'jedi.evaluate.context', 'jedi.evaluate.context.module', 'jedi.evaluate.context.klass', 'jedi.evaluate.context.function', 'jedi.evaluate.docstrings', 'jedi.evaluate.pep0484', 'jedi.evaluate.arguments', 'jedi.evaluate.context.iterable', 'jedi.evaluate.param', 'jedi.evaluate.context.asynchronous', 'jedi.evaluate.parser_cache', 'jedi.evaluate.context.instance', 'jedi.evaluate.syntax_tree', 'jedi.evaluate.finder', 'jedi.api.keywords', 'pydoc_data', 'pydoc_data.topics', 'jedi.api.interpreter', 'jedi.evaluate.compiled.mixed', 'jedi.api.helpers', 'jedi.api.completion', 'jedi.api.environment', 'filecmp', 'jedi.evaluate.compiled.subprocess', 'jedi.evaluate.compiled.subprocess.functions', 'jedi.api.exceptions', 'jedi.api.project', 'jedi.evaluate.usages', 'IPython.terminal.ptutils', 'IPython.terminal.shortcuts', 'IPython.terminal.magics', 'IPython.lib.clipboard', 'IPython.terminal.pt_inputhooks', 'IPython.terminal.prompts', 'IPython.terminal.ipapp', 'IPython.core.magics', 'IPython.core.magics.auto', 'IPython.core.magics.basic', 'IPython.core.magics.code', 'urllib.request', 'email', 'http', 'http.client', 'email.parser', 'email.feedparser', 'email.errors', 'email._policybase', 'email.header', 'email.quoprimime', 'email.base64mime', 'email.charset', 'email.encoders', 'quopri', 'email.utils', 'email._parseaddr', 'calendar', 'email.message', 'uu', 'email._encoded_words', 'email.iterators', 'urllib.error', 'urllib.response', 'IPython.core.magics.config', 'IPython.core.magics.display', 'IPython.core.magics.execution', 'timeit', 'cProfile', '_lsprof', 'profile', 'optparse', 'pstats', 'IPython.utils.module_paths', 'IPython.utils.timing', 'IPython.core.magics.extension', 'IPython.core.magics.history', 'IPython.core.magics.logging', 'IPython.core.magics.namespace', 'IPython.core.magics.osm', 'IPython.core.magics.packaging', 'IPython.core.magics.pylab', 'IPython.core.pylabtools', 'IPython.core.magics.script', 'IPython.lib.backgroundjobs', 'IPython.core.shellapp', 'IPython.extensions', 'IPython.extensions.storemagic', 'IPython.utils.frame', 'jupyter_client', 'jupyter_client._version', 'jupyter_client.connect', 'zmq', 'ctypes', '_ctypes', 'ctypes._endian', 'zmq.backend', 'zmq.backend.select', 'zmq.backend.cython', 'zmq.backend.cython.constants', 'cython_runtime', 'zmq.backend.cython.error', '_cython_0_29_5', 'zmq.backend.cython.message', 'zmq.error', 'zmq.backend.cython.context', 'zmq.backend.cython.socket', 'zmq.backend.cython.utils', 'zmq.backend.cython._poll', 'zmq.backend.cython._version', 'zmq.backend.cython._device', 'zmq.backend.cython._proxy_steerable', 'zmq.sugar', 'zmq.sugar.constants', 'zmq.utils', 'zmq.utils.constant_names', 'zmq.sugar.context', 'zmq.sugar.attrsettr', 'zmq.sugar.socket', 'zmq.sugar.poll', 'zmq.utils.jsonapi', 'zmq.utils.strtypes', 'simplejson', 'decimal', 'numbers', '_decimal', 'simplejson.errors', 'simplejson.raw_json', 'simplejson.decoder', 'simplejson.compat', 'simplejson.scanner', 'simplejson._speedups', 'simplejson.encoder', 'zmq.sugar.frame', 'zmq.sugar.tracker', 'zmq.sugar.version', 'zmq.sugar.stopwatch', 'jupyter_client.localinterfaces', 'jupyter_core', 'jupyter_core.version', 'jupyter_core.paths', 'distutils', 'distutils.util', 'distutils.errors', 'distutils.dep_util', 'distutils.spawn', 'distutils.debug', 'distutils.log', 'jupyter_client.launcher', 'traitlets.log', 'jupyter_client.client', 'jupyter_client.channels', 'jupyter_client.channelsabc', 'jupyter_client.clientabc', 'jupyter_client.manager', 'jupyter_client.kernelspec', 'jupyter_client.managerabc', 'jupyter_client.blocking', 'jupyter_client.blocking.client', 'jupyter_client.blocking.channels', 'jupyter_client.multikernelmanager', 'uuid', 'ctypes.util', 'ipykernel.kernelapp', 'tornado', 'tornado.ioloop', 'tornado.concurrent', 'tornado.log', 'logging.handlers', 'tornado.escape', 'tornado.util', 'tornado.speedups', 'curses', '_curses', 'tornado.stack_context', 'tornado.platform', 'tornado.platform.auto', 'tornado.platform.posix', 'tornado.platform.common', 'tornado.platform.interface', 'zmq.eventloop', 'zmq.eventloop.ioloop', 'tornado.platform.asyncio', 'tornado.gen', 'zmq.eventloop.zmqstream', 'ipykernel.iostream', 'imp', 'jupyter_client.session', 'hmac', 'jupyter_client.jsonutil', 'dateutil', 'dateutil._version', 'dateutil.parser', 'dateutil.parser._parser', 'dateutil.relativedelta', 'dateutil._common', 'dateutil.tz', 'dateutil.tz.tz', 'dateutil.tz._common', 'dateutil.tz._factories', 'dateutil.parser.isoparser', '_strptime', 'jupyter_client.adapter', 'ipykernel.heartbeat', 'ipykernel.ipkernel', 'IPython.utils.tokenutil', 'ipykernel.comm', 'ipykernel.comm.manager', 'ipykernel.comm.comm', 'ipykernel.kernelbase', 'tornado.queues', 'tornado.locks', 'ipykernel.jsonutil', 'ipykernel.zmqshell', 'IPython.core.payloadpage', 'ipykernel.displayhook', 'ipykernel.parentpoller', 'faulthandler', 'ipykernel.datapub', 'ipykernel.serialize', 'ipykernel.pickleutil', 'ipykernel.codeutil', 'IPython.core.completerlib', 'storemagic', 'ipywidgets', 'ipywidgets._version', 'ipywidgets.widgets', 'ipywidgets.widgets.widget', 'ipywidgets.widgets.domwidget', 'ipywidgets.widgets.trait_types', 'ipywidgets.widgets.widget_layout', 'ipywidgets.widgets.widget_style', 'ipywidgets.widgets.valuewidget', 'ipywidgets.widgets.widget_core', 'ipywidgets.widgets.widget_bool', 'ipywidgets.widgets.widget_description', 'ipywidgets.widgets.widget_button', 'ipywidgets.widgets.widget_box', 'ipywidgets.widgets.docutils', 'ipywidgets.widgets.widget_float', 'ipywidgets.widgets.widget_int', 'ipywidgets.widgets.widget_color', 'ipywidgets.widgets.widget_date', 'ipywidgets.widgets.widget_output', 'ipywidgets.widgets.widget_selection', 'ipywidgets.widgets.widget_selectioncontainer', 'ipywidgets.widgets.widget_string', 'ipywidgets.widgets.widget_controller', 'ipywidgets.widgets.interaction', 'ipywidgets.widgets.widget_link', 'ipywidgets.widgets.widget_media', 'numpy', 'numpy._globals', 'numpy.__config__', 'numpy.version', 'numpy._distributor_init', 'numpy.core', 'numpy.core.info', 'numpy.core.multiarray', 'numpy.core.overrides', 'numpy.core._multiarray_umath', 'numpy.compat', 'numpy.compat._inspect', 'numpy.compat.py3k', 'numpy.core.umath', 'numpy.core.numerictypes', 'numpy.core._string_helpers', 'numpy.core._type_aliases', 'numpy.core._dtype', 'numpy.core.numeric', 'numpy.core._internal', 'numpy.core.fromnumeric', 'numpy.core._methods', 'numpy.core.arrayprint', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.function_base', 'numpy.core.machar', 'numpy.core.getlimits', 'numpy.core.shape_base', 'numpy.core.einsumfunc', 'numpy.core._add_newdocs', 'numpy.core._multiarray_tests', 'numpy.core._dtype_ctypes', 'numpy._pytesttester', 'numpy.lib', 'numpy.lib.info', 'numpy.lib.type_check', 'numpy.lib.ufunclike', 'numpy.lib.index_tricks', 'numpy.matrixlib', 'numpy.matrixlib.defmatrix', 'numpy.linalg', 'numpy.linalg.info', 'numpy.linalg.linalg', 'numpy.lib.twodim_base', 'numpy.linalg.lapack_lite', 'numpy.linalg._umath_linalg', 'numpy.lib.function_base', 'numpy.lib.utils', 'numpy.lib.histograms', 'numpy.lib.stride_tricks', 'numpy.lib.mixins', 'numpy.lib.nanfunctions', 'numpy.lib.shape_base', 'numpy.lib.scimath', 'numpy.lib.polynomial', 'numpy.lib.arraysetops', 'numpy.lib.npyio', 'numpy.lib.format', 'numpy.lib._datasource', 'numpy.lib._iotools', 'numpy.lib.financial', 'numpy.lib.arrayterator', 'numpy.lib.arraypad', 'numpy.lib._version', 'numpy.fft', 'numpy.fft.info', 'numpy.fft.fftpack', 'numpy.fft.fftpack_lite', 'numpy.fft.helper', 'numpy.polynomial', 'numpy.polynomial.polynomial', 'numpy.polynomial.polyutils', 'numpy.polynomial._polybase', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermite_e', 'numpy.polynomial.laguerre', 'numpy.random', 'numpy.random.mtrand', 'mtrand', 'numpy.ctypeslib', 'numpy.ma', 'numpy.ma.core', 'numpy.ma.extras', 'numpy.testing', 'unittest', 'unittest.result', 'unittest.util', 'unittest.case', 'unittest.suite', 'unittest.loader', 'unittest.main', 'unittest.runner', 'unittest.signals', 'numpy.testing._private', 'numpy.testing._private.utils', 'numpy.testing._private.decorators', 'numpy.testing._private.nosetester', 'scipy', 'scipy._distributor_init', 'scipy.__config__', 'scipy.version', 'scipy._lib', 'scipy._lib._testutils', 'scipy._lib._version', 'scipy._lib.six', 'scipy._lib._ccallback', 'scipy._lib._ccallback_c', 'scipy.stats', 'scipy.stats.stats', 'scipy._lib._util', 'scipy.special', 'scipy.special.sf_error', 'scipy.special._ufuncs', '_cython_0_29', 'scipy.special._ufuncs_cxx', 'scipy.special.basic', 'scipy.special.specfun', 'scipy.special.orthogonal', 'scipy.linalg', 'scipy.linalg.linalg_version', 'scipy.linalg.misc', 'scipy.linalg.blas', 'scipy.linalg._fblas', 'scipy.linalg.lapack', 'scipy.linalg._flapack', 'scipy.linalg.basic', 'scipy.linalg.flinalg', 'scipy.linalg._flinalg', 'scipy.linalg.decomp', 'scipy.linalg.decomp_svd', 'scipy.linalg._solve_toeplitz', 'scipy.linalg.decomp_lu', 'scipy.linalg._decomp_ldl', 'scipy.linalg.decomp_cholesky', 'scipy.linalg.decomp_qr', 'scipy.linalg._decomp_qz', 'scipy.linalg.decomp_schur', 'scipy.linalg._decomp_polar', 'scipy.linalg.matfuncs', 'scipy.linalg.special_matrices', 'scipy.linalg._expm_frechet', 'scipy.linalg._matfuncs_sqrtm', 'scipy.linalg._solvers', 'scipy.linalg._procrustes', 'scipy.linalg._decomp_update', 'scipy.linalg.cython_blas', 'scipy.linalg.cython_lapack', 'scipy.linalg._sketches', 'numpy.dual', 'scipy.special._comb', 'scipy.special._logsumexp', 'scipy.special.spfun_stats', 'scipy.special._ellip_harm', 'scipy.special._ellip_harm_2', 'scipy.special.lambertw', 'scipy.special._spherical_bessel', 'scipy.stats.distributions', 'scipy.stats._distn_infrastructure', 'scipy.misc', 'scipy.misc.doccer', 'scipy.misc.common', 'scipy.interpolate', 'scipy.interpolate.interpolate', 'scipy.interpolate.fitpack', 'scipy.interpolate._fitpack_impl', 'scipy.interpolate._fitpack', 'scipy.interpolate.dfitpack', 'scipy.interpolate._bsplines', 'scipy.interpolate._bspl', 'scipy.interpolate.polyint', 'scipy.interpolate._ppoly', 'scipy.interpolate.fitpack2', 'scipy.interpolate.interpnd', 'scipy.spatial', 'scipy.spatial.kdtree', 'scipy.sparse', 'scipy.sparse.base', 'scipy._lib._numpy_compat', 'scipy.sparse.sputils', 'scipy.sparse.csr', 'scipy.sparse._sparsetools', 'scipy.sparse.compressed', 'scipy.sparse.data', 'scipy.sparse.dia', 'scipy.sparse.csc', 'scipy.sparse.lil', 'scipy.sparse._csparsetools', 'scipy.sparse.dok', 'scipy.sparse.coo', 'scipy.sparse.bsr', 'scipy.sparse.construct', 'scipy.sparse.extract', 'scipy.sparse._matrix_io', 'scipy.sparse.csgraph', 'scipy.sparse.csgraph._laplacian', 'scipy.sparse.csgraph._shortest_path', 'scipy.sparse.csgraph._validation', 'scipy.sparse.csgraph._tools', 'scipy.sparse.csgraph._traversal', 'scipy.sparse.csgraph._min_spanning_tree', 'scipy.sparse.csgraph._reordering', 'scipy.spatial.ckdtree', 'scipy.spatial.qhull', 'scipy._lib.messagestream', 'scipy.spatial._spherical_voronoi', 'scipy.spatial._voronoi', 'scipy.spatial.distance', 'scipy.spatial._distance_wrap', 'scipy.spatial._hausdorff', 'scipy.spatial._plotutils', 'scipy._lib.decorator', 'scipy.spatial._procrustes', 'scipy.spatial.transform', 'scipy.spatial.transform.rotation', 'scipy.interpolate.rbf', 'scipy.interpolate._cubic', 'scipy.interpolate.ndgriddata', 'scipy.interpolate._pade', 'scipy.misc.pilutil', 'PIL', 'PIL.version', 'PIL.Image', 'PIL._imaging', 'PIL.ImageMode', 'PIL._binary', 'PIL._util', 'PIL.ImageFilter', 'scipy.stats._distr_params', 'scipy.optimize', 'scipy.optimize.optimize', 'scipy.optimize.linesearch', 'scipy.optimize.minpack2', 'scipy.optimize._minimize', 'scipy.optimize._trustregion_dogleg', 'scipy.optimize._trustregion', 'scipy.optimize._trustregion_ncg', 'scipy.optimize._trustregion_krylov', 'scipy.optimize._trlib', 'scipy.optimize._trlib._trlib', 'scipy.optimize._trustregion_exact', 'scipy.optimize._trustregion_constr', 'scipy.optimize._trustregion_constr.minimize_trustregion_constr', 'scipy.sparse.linalg', 'scipy.sparse.linalg.isolve', 'scipy.sparse.linalg.isolve.iterative', 'scipy.sparse.linalg.isolve._iterative', 'scipy.sparse.linalg.interface', 'scipy.sparse.linalg.isolve.utils', 'scipy._lib._threadsafety', 'scipy.sparse.linalg.isolve.minres', 'scipy.sparse.linalg.isolve.lgmres', 'scipy.sparse.linalg.isolve._gcrotmk', 'scipy.sparse.linalg.isolve.lsqr', 'scipy.sparse.linalg.isolve.lsmr', 'scipy.sparse.linalg.dsolve', 'scipy.sparse.linalg.dsolve.linsolve', 'scipy.sparse.linalg.dsolve._superlu', 'scipy.sparse.linalg.dsolve._add_newdocs', 'scipy.sparse.linalg.eigen', 'scipy.sparse.linalg.eigen.arpack', 'scipy.sparse.linalg.eigen.arpack.arpack', 'scipy.sparse.linalg.eigen.arpack._arpack', 'scipy.sparse.linalg.eigen.lobpcg', 'scipy.sparse.linalg.eigen.lobpcg.lobpcg', 'scipy.sparse.linalg.matfuncs', 'scipy.sparse.linalg._onenormest', 'scipy.sparse.linalg._norm', 'scipy.sparse.linalg._expm_multiply', 'scipy.optimize._differentiable_functions', 'scipy.optimize._numdiff', 'scipy.optimize._group_columns', 'scipy.optimize._hessian_update_strategy', 'scipy.optimize._constraints', 'scipy.optimize._trustregion_constr.equality_constrained_sqp', 'scipy.optimize._trustregion_constr.projections', 'scipy.optimize._trustregion_constr.qp_subproblem', 'scipy.optimize._trustregion_constr.canonical_constraint', 'scipy.optimize._trustregion_constr.tr_interior_point', 'scipy.optimize._trustregion_constr.report', 'scipy.optimize.lbfgsb', 'scipy.optimize._lbfgsb', 'scipy.optimize.tnc', 'scipy.optimize.moduleTNC', 'scipy.optimize.cobyla', 'scipy.optimize._cobyla', 'scipy.optimize.slsqp', 'scipy.optimize._slsqp', 'scipy.optimize._root', 'scipy.optimize.minpack', 'scipy.optimize._minpack', 'scipy.optimize._lsq', 'scipy.optimize._lsq.least_squares', 'scipy.optimize._lsq.trf', 'scipy.optimize._lsq.common', 'scipy.optimize._lsq.dogbox', 'scipy.optimize._lsq.lsq_linear', 'scipy.optimize._lsq.trf_linear', 'scipy.optimize._lsq.givens_elimination', 'scipy.optimize._lsq.bvls', 'scipy.optimize._spectral', 'scipy.optimize.nonlin', 'scipy.optimize._root_scalar', 'scipy.optimize.zeros', 'scipy.optimize._zeros', 'scipy.optimize.nnls', 'scipy.optimize._nnls', 'scipy.optimize._basinhopping', 'scipy.optimize._linprog', 'scipy.optimize._linprog_ip', 'scipy.optimize._linprog_simplex', 'scipy.optimize._linprog_util', 'scipy.optimize._remove_redundancy', 'scipy.optimize._hungarian', 'scipy.optimize._differentialevolution', 'scipy.optimize._shgo', 'scipy.optimize._shgo_lib', 'scipy.optimize._shgo_lib.sobol_seq', 'scipy.optimize._shgo_lib.triangulation', 'scipy.optimize._dual_annealing', 'scipy.integrate', 'scipy.integrate.quadrature', 'scipy.integrate.odepack', 'scipy.integrate._odepack', 'scipy.integrate.quadpack', 'scipy.integrate._quadpack', 'scipy.integrate._ode', 'scipy.integrate.vode', 'scipy.integrate._dop', 'scipy.integrate.lsoda', 'scipy.integrate._bvp', 'scipy.integrate._ivp', 'scipy.integrate._ivp.ivp', 'scipy.integrate._ivp.bdf', 'scipy.integrate._ivp.common', 'scipy.integrate._ivp.base', 'scipy.integrate._ivp.radau', 'scipy.integrate._ivp.rk', 'scipy.integrate._ivp.lsoda', 'scipy.stats._constants', 'scipy.stats._continuous_distns', 'scipy.stats._stats', 'scipy.stats._tukeylambda_stats', 'scipy.stats._discrete_distns', 'scipy.stats.mstats_basic', 'scipy.stats._stats_mstats_common', 'scipy.stats._rvs_sampling', 'scipy.stats.morestats', 'scipy.stats.statlib', 'scipy.stats.contingency', 'scipy.stats._binned_statistic', 'scipy.stats.kde', 'scipy.stats.mvn', 'scipy.stats.mstats', 'scipy.stats.mstats_extras', 'scipy.stats._multivariate', 'torch', 'torch._utils', 'torch._utils_internal', 'torch.version', 'torch._six', 'torch._C._onnx', 'torch._C._jit_tree_views', 'torch._C.cpp', 'torch._C.cpp.nn', 'torch._C._cudart', 'torch._C._nvtx', 'torch._C._cudnn', 'torch._C', 'torch.random', 'torch.serialization', 'tarfile', 'torch._tensor_str', 'torch.tensor', 'torch._namedtensor_internals', 'torch.utils', 'torch.utils.throughput_benchmark', 'torch.utils.hooks', 'torch.storage', 'torch.cuda', 'torch.cuda._utils', 'torch.types', 'torch.cuda.streams', 'torch.cuda.memory', 'torch.cuda.random', 'torch.cuda.sparse', 'torch.cuda.profiler', 'torch.cuda.nvtx', 'torch.cuda.amp', 'torch.cuda.amp.autocast_mode', 'torch.cuda.amp.grad_scaler', 'torch.sparse', 'torch.functional', 'torch.nn', 'torch.nn.modules', 'torch.nn.modules.module', 'torch.nn.parameter', 'torch.nn.modules.linear', 'torch.nn.functional', 'torch.nn._reduction', 'torch.nn.modules.utils', 'torch.nn.grad', 'torch._VF', 'torch._jit_internal', 'torch.distributed', 'torch.distributed.distributed_c10d', 'torch.distributed.constants', 'torch.distributed.rendezvous', 'torch.distributed.rpc', 'torch.distributed.rpc.api', 'torch.distributed.rpc.backend_registry', 'torch.distributed.rpc.constants', 'torch.distributed.rpc.internal', 'torch.distributed.rpc.functions', 'torch.distributed.rpc.server_process_global_profiler', 'torch.autograd', 'torch.autograd.variable', 'torch.autograd.function', 'torch.autograd.gradcheck', 'torch.testing', 'torch.autograd.grad_mode', 'torch.autograd.anomaly_mode', 'torch.autograd.profiler', 'torch.autograd.functional', 'torch.distributed.autograd', 'typing_extensions', 'torch._overrides', 'torch.nn.init', 'torch.nn.modules.conv', 'torch.nn.common_types', 'torch.nn.modules.activation', 'torch.nn.modules.loss', 'torch.nn.modules.container', 'torch.nn.modules.pooling', 'torch.nn.modules.batchnorm', 'torch.nn.modules._functions', 'torch.nn.modules.instancenorm', 'torch.nn.modules.normalization', 'torch.nn.modules.dropout', 'torch.nn.modules.padding', 'torch.nn.modules.sparse', 'torch.nn.modules.rnn', 'torch.nn.utils', 'torch.nn.utils.rnn', 'torch.nn.utils.clip_grad', 'torch.nn.utils.weight_norm', 'torch.nn.utils.convert_parameters', 'torch.nn.utils.spectral_norm', 'torch.nn.utils.fusion', 'torch.nn.utils.memory_format', 'torch.nn.modules.pixelshuffle', 'torch.nn.modules.upsampling', 'torch.nn.modules.distance', 'torch.nn.modules.fold', 'torch.nn.modules.adaptive', 'torch.nn.modules.transformer', 'torch.nn.modules.flatten', 'torch.nn.parallel', 'torch.nn.parallel.parallel_apply', 'torch.nn.parallel.replicate', 'torch.cuda.comm', 'torch.cuda.nccl', 'torch.nn.parallel.data_parallel', 'torch.nn.parallel.scatter_gather', 'torch.nn.parallel._functions', 'torch.nn.parallel.distributed', 'torch._lowrank', 'torch._linalg_utils', 'torch.futures', 'torch.nn.intrinsic', 'torch.nn.intrinsic.modules', 'torch.nn.intrinsic.modules.fused', 'torch.nn.quantized', 'torch.nn.quantized.modules', 'torch.nn.quantized.modules.activation', 'torch.nn.quantized.functional', 'torch.nn.quantized.modules.utils', 'torch.nn.quantized.modules.batchnorm', 'torch.nn.quantized.modules.normalization', 'torch.nn.quantized.modules.conv', 'torch.nn.intrinsic.qat', 'torch.nn.intrinsic.qat.modules', 'torch.nn.intrinsic.qat.modules.linear_relu', 'torch.nn.qat', 'torch.nn.qat.modules', 'torch.nn.qat.modules.linear', 'torch.nn.qat.modules.conv', 'torch.nn.intrinsic.qat.modules.conv_fused', 'torch._ops', 'torch.jit', 'torch.jit.annotations', 'torch.jit._recursive', 'torch.jit.frontend', 'torch.jit._builtins', 'torch.backends', 'torch.backends.cudnn', 'torch.nn.quantized.modules.linear', 'torch.nn.quantized.modules.functional_modules', 'torch.optim', 'torch.optim.adadelta', 'torch.optim.optimizer', 'torch.optim.adagrad', 'torch.optim.adam', 'torch.optim.adamw', 'torch.optim.sparse_adam', 'torch.optim.adamax', 'torch.optim.asgd', 'torch.optim.sgd', 'torch.optim.rprop', 'torch.optim.rmsprop', 'torch.optim.lbfgs', 'torch.optim.lr_scheduler', 'torch.multiprocessing', 'torch.multiprocessing.reductions', 'multiprocessing.resource_sharer', 'torch.multiprocessing.spawn', 'torch.utils.backcompat', 'torch.onnx', 'torch.hub', 'zipfile', 'torch.distributions', 'torch.distributions.bernoulli', 'torch.distributions.constraints', 'torch.distributions.exp_family', 'torch.distributions.distribution', 'torch.distributions.utils', 'torch.distributions.beta', 'torch.distributions.dirichlet', 'torch.distributions.binomial', 'torch.distributions.categorical', 'torch.distributions.cauchy', 'torch.distributions.chi2', 'torch.distributions.gamma', 'torch.distributions.constraint_registry', 'torch.distributions.transforms', 'torch.distributions.continuous_bernoulli', 'torch.distributions.exponential', 'torch.distributions.fishersnedecor', 'torch.distributions.geometric', 'torch.distributions.gumbel', 'torch.distributions.uniform', 'torch.distributions.transformed_distribution', 'torch.distributions.half_cauchy', 'torch.distributions.half_normal', 'torch.distributions.normal', 'torch.distributions.independent', 'torch.distributions.kl', 'torch.distributions.laplace', 'torch.distributions.lowrank_multivariate_normal', 'torch.distributions.multivariate_normal', 'torch.distributions.one_hot_categorical', 'torch.distributions.pareto', 'torch.distributions.poisson', 'torch.distributions.log_normal', 'torch.distributions.logistic_normal', 'torch.distributions.mixture_same_family', 'torch.distributions.multinomial', 'torch.distributions.negative_binomial', 'torch.distributions.relaxed_bernoulli', 'torch.distributions.relaxed_categorical', 'torch.distributions.studentT', 'torch.distributions.von_mises', 'torch.distributions.weibull', 'torch.backends.cuda', 'torch.backends.mkl', 'torch.backends.mkldnn', 'torch.backends.openmp', 'torch.backends.quantized', 'torch.quantization', 'torch.quantization.quantize', 'torch.quantization.default_mappings', 'torch.nn.intrinsic.quantized', 'torch.nn.intrinsic.quantized.modules', 'torch.nn.intrinsic.quantized.modules.linear_relu', 'torch.nn.intrinsic.quantized.modules.conv_relu', 'torch.nn.intrinsic.quantized.modules.bn_relu', 'torch.nn.quantized.dynamic', 'torch.nn.quantized.dynamic.modules', 'torch.nn.quantized.dynamic.modules.linear', 'torch.nn.quantized.dynamic.modules.rnn', 'torch.quantization.stubs', 'torch.quantization.qconfig', 'torch.quantization.observer', 'torch.quantization.fake_quantize', 'torch.quantization.fuse_modules', 'torch.quantization.quantize_jit', 'torch.utils.data', 'torch.utils.data.sampler', 'torch.utils.data.distributed', 'torch.utils.data.dataset', 'torch.utils.data.dataloader', 'torch.utils.data._utils', 'torch.utils.data._utils.worker', 'torch.utils.data._utils.signal_handling', 'torch.utils.data._utils.pin_memory', 'torch.utils.data._utils.collate', 'torch.utils.data._utils.fetch', 'torch.__config__', 'torch.__future__', 'torch._torch_docs', 'torch._tensor_docs', 'torch._storage_docs', 'torch._classes', 'torch.quasirandom', 'torch.multiprocessing._atfork', 'torch._lobpcg', 'gpytorch', 'gpytorch.beta_features', 'gpytorch.settings', 'gpytorch.distributions', 'gpytorch.distributions.delta', 'gpytorch.distributions.distribution', 'gpytorch.distributions.multivariate_normal', 'gpytorch.lazy', 'gpytorch.lazy.added_diag_lazy_tensor', 'gpytorch.utils', 'gpytorch.utils.broadcasting', 'gpytorch.utils.cholesky', 'gpytorch.utils.errors', 'gpytorch.utils.warnings', 'gpytorch.utils.fft', 'gpytorch.utils.grid', 'gpytorch.utils.interpolation', 'gpytorch.utils.lanczos', 'gpytorch.utils.pivoted_cholesky', 'gpytorch.utils.quadrature', 'gpytorch.utils.sparse', 'gpytorch.utils.contour_integral_quad', 'gpytorch.utils.linear_cg', 'gpytorch.utils.deprecation', 'unittest.mock', 'gpytorch.utils.minres', 'gpytorch.utils.memoize', 'gpytorch.utils.stochastic_lq', 'gpytorch.lazy.diag_lazy_tensor', 'gpytorch.lazy.lazy_tensor', 'gpytorch.functions', 'gpytorch.functions._dsmm', 'gpytorch.functions._log_normal_cdf', 'gpytorch.functions.matern_covariance', 'gpytorch.functions.rbf_covariance', 'gpytorch.functions._inv_matmul', 'gpytorch.functions._inv_quad', 'gpytorch.functions._inv_quad_log_det', 'gpytorch.functions._matmul', 'gpytorch.functions._root_decomposition', 'gpytorch.functions._sqrt_inv_matmul', 'gpytorch.utils.getitem', 'gpytorch.lazy.lazy_tensor_representation_tree', 'gpytorch.lazy.non_lazy_tensor', 'gpytorch.lazy.triangular_lazy_tensor', 'gpytorch.lazy.batch_repeat_lazy_tensor', 'gpytorch.lazy.psd_sum_lazy_tensor', 'gpytorch.lazy.sum_lazy_tensor', 'gpytorch.lazy.zero_lazy_tensor', 'gpytorch.lazy.root_lazy_tensor', 'gpytorch.lazy.matmul_lazy_tensor', 'gpytorch.lazy.block_diag_lazy_tensor', 'gpytorch.lazy.block_lazy_tensor', 'gpytorch.lazy.block_interleaved_lazy_tensor', 'gpytorch.lazy.cached_cg_lazy_tensor', 'gpytorch.lazy.chol_lazy_tensor', 'gpytorch.lazy.cat_lazy_tensor', 'gpytorch.lazy.constant_mul_lazy_tensor', 'gpytorch.lazy.interpolated_lazy_tensor', 'gpytorch.lazy.keops_lazy_tensor', 'gpytorch.lazy.kronecker_product_added_diag_lazy_tensor', 'gpytorch.lazy.kronecker_product_lazy_tensor', 'gpytorch.lazy.lazy_evaluated_kernel_tensor', 'gpytorch.lazy.mul_lazy_tensor', 'gpytorch.lazy.sum_batch_lazy_tensor', 'gpytorch.lazy.toeplitz_lazy_tensor', 'gpytorch.utils.toeplitz', 'gpytorch.distributions.multitask_multivariate_normal', 'gpytorch.kernels', 'gpytorch.kernels.keops', 'gpytorch.kernels.keops.matern_kernel', 'gpytorch.kernels.keops.keops_kernel', 'gpytorch.kernels.kernel', 'gpytorch.constraints', 'gpytorch.constraints.constraints', 'gpytorch.utils.transforms', 'gpytorch.models', 'gpytorch.models.deep_gps', 'gpytorch.models.deep_gps.deep_gp', 'gpytorch.likelihoods', 'gpytorch.likelihoods.bernoulli_likelihood', 'gpytorch.likelihoods.likelihood', 'gpytorch.module', 'gpytorch.likelihoods.beta_likelihood', 'gpytorch.likelihoods.gaussian_likelihood', 'gpytorch.likelihoods.noise_models', 'gpytorch.likelihoods.laplace_likelihood', 'gpytorch.likelihoods.likelihood_list', 'gpytorch.likelihoods.multitask_gaussian_likelihood', 'gpytorch.likelihoods.softmax_likelihood', 'gpytorch.likelihoods.student_t_likelihood', 'gpytorch.models.approximate_gp', 'gpytorch.models.gp', 'gpytorch.models.pyro', 'gpytorch.models.exact_gp', 'gpytorch.models.exact_prediction_strategies', 'gpytorch.models.model_list', 'gpytorch.kernels.keops.rbf_kernel', 'gpytorch.kernels.rbf_kernel', 'gpytorch.kernels.additive_structure_kernel', 'gpytorch.kernels.arc_kernel', 'gpytorch.priors', 'gpytorch.priors.horseshoe_prior', 'gpytorch.priors.prior', 'gpytorch.priors.lkj_prior', 'gpytorch.priors.smoothed_box_prior', 'gpytorch.priors.torch_priors', 'gpytorch.priors.utils', 'gpytorch.kernels.cosine_kernel', 'gpytorch.kernels.cylindrical_kernel', 'gpytorch.kernels.distributional_input_kernel', 'gpytorch.kernels.gaussian_symmetrized_kl_kernel', 'gpytorch.kernels.grid_interpolation_kernel', 'gpytorch.kernels.grid_kernel', 'gpytorch.kernels.index_kernel', 'gpytorch.kernels.inducing_point_kernel', 'gpytorch.mlls', 'gpytorch.mlls.added_loss_term', 'gpytorch.mlls.deep_approximate_mll', 'gpytorch.mlls._approximate_mll', 'gpytorch.mlls.marginal_log_likelihood', 'gpytorch.mlls.deep_predictive_log_likelihood', 'gpytorch.models.deep_gps.dspp', 'gpytorch.mlls.exact_marginal_log_likelihood', 'gpytorch.mlls.gamma_robust_variational_elbo', 'gpytorch.mlls.inducing_point_kernel_added_loss_term', 'gpytorch.mlls.noise_model_added_loss_term', 'gpytorch.mlls.predictive_log_likelihood', 'gpytorch.mlls.sum_marginal_log_likelihood', 'gpytorch.mlls.variational_elbo', 'gpytorch.kernels.lcm_kernel', 'gpytorch.kernels.multitask_kernel', 'gpytorch.kernels.linear_kernel', 'gpytorch.kernels.matern_kernel', 'gpytorch.kernels.multi_device_kernel', 'gpytorch.kernels.newton_girard_additive_kernel', 'gpytorch.kernels.periodic_kernel', 'gpytorch.kernels.polynomial_kernel', 'gpytorch.kernels.polynomial_kernel_grad', 'gpytorch.kernels.product_structure_kernel', 'gpytorch.kernels.rbf_kernel_grad', 'gpytorch.kernels.rff_kernel', 'gpytorch.kernels.rq_kernel', 'gpytorch.kernels.scale_kernel', 'gpytorch.kernels.spectral_delta_kernel', 'gpytorch.kernels.spectral_mixture_kernel', 'gpytorch.means', 'gpytorch.means.constant_mean', 'gpytorch.means.mean', 'gpytorch.means.constant_mean_grad', 'gpytorch.means.linear_mean', 'gpytorch.means.multitask_mean', 'gpytorch.means.zero_mean', 'gpytorch.optim', 'gpytorch.optim.ngd', 'gpytorch.variational', 'gpytorch.variational._variational_distribution', 'gpytorch.variational._variational_strategy', 'gpytorch.variational.additive_grid_interpolation_variational_strategy', 'gpytorch.variational.grid_interpolation_variational_strategy', 'gpytorch.variational.batch_decoupled_variational_strategy', 'gpytorch.variational.delta_variational_distribution', 'gpytorch.variational.variational_strategy', 'gpytorch.variational.cholesky_variational_distribution', 'gpytorch.variational.independent_multitask_variational_strategy', 'gpytorch.variational.lmc_variational_strategy', 'gpytorch.variational.mean_field_variational_distribution', 'gpytorch.variational.natural_variational_distribution', 'gpytorch.variational.orthogonally_decoupled_variational_strategy', 'gpytorch.variational.tril_natural_variational_distribution', 'gpytorch.variational.unwhitened_variational_strategy', 'gpytorch.variational.whitened_variational_strategy', 'gzip', 'matplotlib', 'distutils.version', 'matplotlib.cbook', 'matplotlib.cbook.deprecation', 'matplotlib.rcsetup', 'matplotlib.fontconfig_pattern', 'pyparsing', 'matplotlib.colors', 'matplotlib._color_data', 'cycler', 'matplotlib._version']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib:CACHEDIR=/home/atroska/.cache/matplotlib\n",
      "DEBUG:matplotlib.font_manager:Using fontManager instance from /home/atroska/.cache/matplotlib/fontlist-v300.json\n",
      "DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data sets beacause we previously cast them into Tensor\n",
    "train_x_name = \"train_x.csv\"\n",
    "train_y_name = \"train_y.csv\"\n",
    "\n",
    "train_x = np.loadtxt(train_x_name, delimiter=',')\n",
    "train_y = np.loadtxt(train_y_name, delimiter=',')    \n",
    "\n",
    "# load the test dateset\n",
    "test_x_name = \"test_x.csv\"\n",
    "test_x = np.loadtxt(test_x_name, delimiter=',')\n",
    "\n",
    "# PLOT DATA\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(train_x[:,0], train_x[:,1], train_y, c=train_y, cmap='Greens');    \n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "plt.show()\n",
    "fig.savefig('visual.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visualization of the training set:\n",
    "\n",
    "![image](visual.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT LOSS\n",
    "loss_50_RBF = [0.7372838258743286,\n",
    " 0.7336408495903015,\n",
    " 0.7300812005996704,\n",
    " 0.7264455556869507,\n",
    " 0.7227832674980164,\n",
    " 0.7192136645317078,\n",
    " 0.7155143022537231,\n",
    " 0.7118554711341858,\n",
    " 0.7082007527351379,\n",
    " 0.7045102715492249,\n",
    " 0.7008339762687683,\n",
    " 0.6971420645713806,\n",
    " 0.6934177875518799,\n",
    " 0.6898286938667297,\n",
    " 0.6860259175300598,\n",
    " 0.6822872757911682,\n",
    " 0.6785528063774109,\n",
    " 0.67481929063797,\n",
    " 0.671066403388977,\n",
    " 0.667362630367279,\n",
    " 0.663547694683075,\n",
    " 0.6597496867179871,\n",
    " 0.6559280753135681,\n",
    " 0.6521740555763245,\n",
    " 0.6483582854270935,\n",
    " 0.6444932222366333,\n",
    " 0.6407132148742676,\n",
    " 0.636863112449646,\n",
    " 0.6329894661903381,\n",
    " 0.6291766166687012,\n",
    " 0.6252681016921997,\n",
    " 0.6213940382003784,\n",
    " 0.6174920201301575,\n",
    " 0.6136091351509094,\n",
    " 0.6096917986869812,\n",
    " 0.6057572364807129,\n",
    " 0.6018401384353638,\n",
    " 0.5979123711585999,\n",
    " 0.5939492583274841,\n",
    " 0.5899916291236877,\n",
    " 0.586005449295044,\n",
    " 0.5820373296737671,\n",
    " 0.5780435800552368,\n",
    " 0.5740146040916443,\n",
    " 0.5700116753578186,\n",
    " 0.5660097599029541,\n",
    " 0.5619293451309204,\n",
    " 0.5579075813293457,\n",
    " 0.5538445115089417,\n",
    " 0.5497879385948181]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_200_RBF = [0.7407978773117065,\n",
    " 0.737268328666687,\n",
    " 0.7336359024047852,\n",
    " 0.7300554513931274,\n",
    " 0.7264373302459717,\n",
    " 0.7227652668952942,\n",
    " 0.7191736698150635,\n",
    " 0.7155269980430603,\n",
    " 0.7118185758590698,\n",
    " 0.7082059383392334,\n",
    " 0.704554557800293,\n",
    " 0.7008066773414612,\n",
    " 0.6971200704574585,\n",
    " 0.6934344172477722,\n",
    " 0.689768373966217,\n",
    " 0.6860253810882568,\n",
    " 0.6822997331619263,\n",
    " 0.6785236597061157,\n",
    " 0.674807608127594,\n",
    " 0.6710321307182312,\n",
    " 0.6673129796981812,\n",
    " 0.6635303497314453,\n",
    " 0.659728467464447,\n",
    " 0.6559441089630127,\n",
    " 0.65214604139328,\n",
    " 0.6483465433120728,\n",
    " 0.6445245742797852,\n",
    " 0.6406869292259216,\n",
    " 0.6368102431297302,\n",
    " 0.6330245733261108,\n",
    " 0.629112720489502,\n",
    " 0.6252515912055969,\n",
    " 0.6213547587394714,\n",
    " 0.6174572110176086,\n",
    " 0.6135584712028503,\n",
    " 0.6096677184104919,\n",
    " 0.6057484149932861,\n",
    " 0.6017941832542419,\n",
    " 0.5979022979736328,\n",
    " 0.5939054489135742,\n",
    " 0.5899369120597839,\n",
    " 0.585981011390686,\n",
    " 0.5819771885871887,\n",
    " 0.5779861211776733,\n",
    " 0.5740030407905579,\n",
    " 0.5699904561042786,\n",
    " 0.5659512281417847,\n",
    " 0.5619300007820129,\n",
    " 0.5578595399856567,\n",
    " 0.5538448691368103,\n",
    " 0.5497607588768005,\n",
    " 0.5456945896148682,\n",
    " 0.5415965914726257,\n",
    " 0.53751540184021,\n",
    " 0.5334169864654541,\n",
    " 0.5292996764183044,\n",
    " 0.5251991748809814,\n",
    " 0.5210481286048889,\n",
    " 0.5169076323509216,\n",
    " 0.5127673149108887,\n",
    " 0.5086112022399902,\n",
    " 0.5044301748275757,\n",
    " 0.5002514719963074,\n",
    " 0.4960775673389435,\n",
    " 0.49185794591903687,\n",
    " 0.4876314401626587,\n",
    " 0.48344212770462036,\n",
    " 0.4792076647281647,\n",
    " 0.474959135055542,\n",
    " 0.47070571780204773,\n",
    " 0.4664444923400879,\n",
    " 0.4621712565422058,\n",
    " 0.45789679884910583,\n",
    " 0.45361441373825073,\n",
    " 0.4493216574192047,\n",
    " 0.44503113627433777,\n",
    " 0.4407012462615967,\n",
    " 0.436390221118927,\n",
    " 0.43205735087394714,\n",
    " 0.4277353882789612,\n",
    " 0.4233672022819519,\n",
    " 0.41901952028274536,\n",
    " 0.4146497845649719,\n",
    " 0.4102838635444641,\n",
    " 0.40590420365333557,\n",
    " 0.4014984667301178,\n",
    " 0.3971039950847626,\n",
    " 0.39271852374076843,\n",
    " 0.3882681727409363,\n",
    " 0.38386207818984985,\n",
    " 0.3794240951538086,\n",
    " 0.3750030994415283,\n",
    " 0.37052005529403687,\n",
    " 0.36608609557151794,\n",
    " 0.36160242557525635,\n",
    " 0.3571377396583557,\n",
    " 0.3526517152786255,\n",
    " 0.3481546640396118,\n",
    " 0.34367918968200684,\n",
    " 0.3391571044921875,\n",
    " 0.33465510606765747,\n",
    " 0.33014780282974243,\n",
    " 0.3256126642227173,\n",
    " 0.321075975894928,\n",
    " 0.3165452480316162,\n",
    " 0.3119962513446808,\n",
    " 0.307426393032074,\n",
    " 0.30286136269569397,\n",
    " 0.29832321405410767,\n",
    " 0.29372045397758484,\n",
    " 0.2891295254230499,\n",
    " 0.2845715582370758,\n",
    " 0.2799679636955261,\n",
    " 0.2753795385360718,\n",
    " 0.270751416683197,\n",
    " 0.2661421597003937,\n",
    " 0.26152503490448,\n",
    " 0.2569270133972168,\n",
    " 0.25227418541908264,\n",
    " 0.24762636423110962,\n",
    " 0.2429981827735901,\n",
    " 0.2383105754852295,\n",
    " 0.23367029428482056,\n",
    " 0.22902309894561768,\n",
    " 0.22433128952980042,\n",
    " 0.21966344118118286,\n",
    " 0.21499383449554443,\n",
    " 0.21031968295574188,\n",
    " 0.20560020208358765,\n",
    " 0.20093858242034912,\n",
    " 0.19625164568424225,\n",
    " 0.19149190187454224,\n",
    " 0.18677592277526855,\n",
    " 0.18215329945087433,\n",
    " 0.17739152908325195,\n",
    " 0.17263105511665344,\n",
    " 0.16788949072360992,\n",
    " 0.16314741969108582,\n",
    " 0.15840160846710205,\n",
    " 0.15373171865940094,\n",
    " 0.1488761305809021,\n",
    " 0.1441163420677185,\n",
    " 0.1393495798110962,\n",
    " 0.13458973169326782,\n",
    " 0.1297823190689087,\n",
    " 0.1250573992729187,\n",
    " 0.12025079131126404,\n",
    " 0.11544536799192429,\n",
    " 0.11067957431077957,\n",
    " 0.10583197325468063,\n",
    " 0.10106001049280167,\n",
    " 0.0962940976023674,\n",
    " 0.09145782142877579,\n",
    " 0.0866178646683693,\n",
    " 0.08181878179311752,\n",
    " 0.07695385813713074,\n",
    " 0.07217980176210403,\n",
    " 0.06732229143381119,\n",
    " 0.06250577419996262,\n",
    " 0.05770006775856018,\n",
    " 0.052883829921483994,\n",
    " 0.04803645983338356,\n",
    " 0.04319276660680771,\n",
    " 0.03832540661096573,\n",
    " 0.033520832657814026,\n",
    " 0.028598900884389877,\n",
    " 0.023771965876221657,\n",
    " 0.018945256248116493,\n",
    " 0.014093070290982723,\n",
    " 0.00920997466892004,\n",
    " 0.004313971847295761,\n",
    " -0.0004942255327478051,\n",
    " -0.005360054317861795,\n",
    " -0.010288779623806477,\n",
    " -0.015134171582758427,\n",
    " -0.020009057596325874,\n",
    " -0.02488507702946663,\n",
    " -0.029743093997240067,\n",
    " -0.034722261130809784,\n",
    " -0.039593297988176346,\n",
    " -0.0445685014128685,\n",
    " -0.04934103414416313,\n",
    " -0.054283853620290756,\n",
    " -0.05929313972592354,\n",
    " -0.06419350206851959,\n",
    " -0.06923460215330124,\n",
    " -0.07391644269227982,\n",
    " -0.07894112169742584,\n",
    " -0.08397260308265686,\n",
    " -0.08891429007053375,\n",
    " -0.09405774623155594,\n",
    " -0.09911379218101501,\n",
    " -0.10410473495721817,\n",
    " -0.10905174165964127,\n",
    " -0.11405853927135468,\n",
    " -0.11902479827404022,\n",
    " -0.12382020056247711,\n",
    " -0.12918896973133087,\n",
    " -0.13391520082950592,\n",
    " -0.13888303935527802]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_50_RQ = [0.7408291697502136,\n",
    " 0.7373445630073547,\n",
    " 0.7337273359298706,\n",
    " 0.7301963567733765,\n",
    " 0.7265686988830566,\n",
    " 0.7229055762290955,\n",
    " 0.7192351818084717,\n",
    " 0.7154425382614136,\n",
    " 0.7118580937385559,\n",
    " 0.7081517577171326,\n",
    " 0.7045953273773193,\n",
    " 0.7007681131362915,\n",
    " 0.6972112655639648,\n",
    " 0.6935121417045593,\n",
    " 0.6897627115249634,\n",
    " 0.6860882043838501,\n",
    " 0.6824801564216614,\n",
    " 0.6784180998802185,\n",
    " 0.6748413443565369,\n",
    " 0.6711008548736572,\n",
    " 0.6672126650810242,\n",
    " 0.6634678244590759,\n",
    " 0.6596406102180481,\n",
    " 0.6558593511581421,\n",
    " 0.6521669030189514,\n",
    " 0.648127019405365,\n",
    " 0.6447355151176453,\n",
    " 0.6406757235527039,\n",
    " 0.6367630362510681,\n",
    " 0.6329981684684753,\n",
    " 0.6290362477302551,\n",
    " 0.6252118349075317,\n",
    " 0.6213237643241882,\n",
    " 0.6174219250679016,\n",
    " 0.613435685634613,\n",
    " 0.6096569299697876,\n",
    " 0.6055914759635925,\n",
    " 0.6017864346504211,\n",
    " 0.5978978872299194,\n",
    " 0.5937071442604065,\n",
    " 0.5899136066436768,\n",
    " 0.5858387351036072,\n",
    " 0.5820266604423523,\n",
    " 0.577890157699585,\n",
    " 0.5739046931266785,\n",
    " 0.5699408054351807,\n",
    " 0.565947949886322,\n",
    " 0.5617966651916504,\n",
    " 0.5578041672706604,\n",
    " 0.5536976456642151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_200_RQ = [0.7410196661949158,\n",
    " 0.737395703792572,\n",
    " 0.7338513135910034,\n",
    " 0.730100691318512,\n",
    " 0.7264084219932556,\n",
    " 0.7228573560714722,\n",
    " 0.7190613746643066,\n",
    " 0.7155680060386658,\n",
    " 0.7118827104568481,\n",
    " 0.7082642316818237,\n",
    " 0.7045201659202576,\n",
    " 0.7007850408554077,\n",
    " 0.6972015500068665,\n",
    " 0.6934447288513184,\n",
    " 0.6896883249282837,\n",
    " 0.6861289143562317,\n",
    " 0.6824561357498169,\n",
    " 0.6785658001899719,\n",
    " 0.6748430132865906,\n",
    " 0.6710515022277832,\n",
    " 0.6673564314842224,\n",
    " 0.6635303497314453,\n",
    " 0.6596902012825012,\n",
    " 0.6558852195739746,\n",
    " 0.6520410776138306,\n",
    " 0.6482564806938171,\n",
    " 0.6444346904754639,\n",
    " 0.6405996680259705,\n",
    " 0.6367067694664001,\n",
    " 0.6329953670501709,\n",
    " 0.6290954351425171,\n",
    " 0.6251645684242249,\n",
    " 0.6213158965110779,\n",
    " 0.6174340844154358,\n",
    " 0.6135520935058594,\n",
    " 0.6096988916397095,\n",
    " 0.6056192517280579,\n",
    " 0.601702094078064,\n",
    " 0.5976876020431519,\n",
    " 0.5938392281532288,\n",
    " 0.5899083614349365,\n",
    " 0.585891842842102,\n",
    " 0.5818201899528503,\n",
    " 0.5778283476829529,\n",
    " 0.5739632248878479,\n",
    " 0.5699422359466553,\n",
    " 0.5658557415008545,\n",
    " 0.5618480443954468,\n",
    " 0.5576963424682617,\n",
    " 0.5537614822387695,\n",
    " 0.5496633648872375,\n",
    " 0.5455306172370911,\n",
    " 0.5414003133773804,\n",
    " 0.5373219847679138,\n",
    " 0.5332559943199158,\n",
    " 0.529159426689148,\n",
    " 0.5250018239021301,\n",
    " 0.5210347771644592,\n",
    " 0.5167162418365479,\n",
    " 0.5125724077224731,\n",
    " 0.5084154009819031,\n",
    " 0.5043051242828369,\n",
    " 0.5000858902931213,\n",
    " 0.49587512016296387,\n",
    " 0.4917609691619873,\n",
    " 0.4874749779701233,\n",
    " 0.4832253158092499,\n",
    " 0.47891825437545776,\n",
    " 0.4747765362262726,\n",
    " 0.47044315934181213,\n",
    " 0.46629253029823303,\n",
    " 0.4619961380958557,\n",
    " 0.45755067467689514,\n",
    " 0.4532465934753418,\n",
    " 0.44909313321113586,\n",
    " 0.44478532671928406,\n",
    " 0.44046908617019653,\n",
    " 0.43615829944610596,\n",
    " 0.4317442774772644,\n",
    " 0.42738014459609985,\n",
    " 0.42306986451148987,\n",
    " 0.41873833537101746,\n",
    " 0.4145037829875946,\n",
    " 0.40993669629096985,\n",
    " 0.4055618345737457,\n",
    " 0.40106794238090515,\n",
    " 0.3968156576156616,\n",
    " 0.39237144589424133,\n",
    " 0.3878096640110016,\n",
    " 0.3834761083126068,\n",
    " 0.37899139523506165,\n",
    " 0.3746238648891449,\n",
    " 0.3699914515018463,\n",
    " 0.3655812442302704,\n",
    " 0.3610653281211853,\n",
    " 0.35674479603767395,\n",
    " 0.35219934582710266,\n",
    " 0.3476150333881378,\n",
    " 0.34319111704826355,\n",
    " 0.3386653661727905,\n",
    " 0.3341696560382843,\n",
    " 0.3296106159687042,\n",
    " 0.32492250204086304,\n",
    " 0.320463091135025,\n",
    " 0.31589871644973755,\n",
    " 0.31140241026878357,\n",
    " 0.3069106638431549,\n",
    " 0.3021818995475769,\n",
    " 0.2978302240371704,\n",
    " 0.29305145144462585,\n",
    " 0.28855276107788086,\n",
    " 0.2838907241821289,\n",
    " 0.279284805059433,\n",
    " 0.27454614639282227,\n",
    " 0.27010437846183777,\n",
    " 0.26534345746040344,\n",
    " 0.26087087392807007,\n",
    " 0.25611111521720886,\n",
    " 0.2513771057128906,\n",
    " 0.24664565920829773,\n",
    " 0.2419988065958023,\n",
    " 0.23729750514030457,\n",
    " 0.23277586698532104,\n",
    " 0.22813546657562256,\n",
    " 0.22327066957950592,\n",
    " 0.21858571469783783,\n",
    " 0.21384307742118835,\n",
    " 0.2091580033302307,\n",
    " 0.20454749464988708,\n",
    " 0.19988705217838287,\n",
    " 0.1952030062675476,\n",
    " 0.19031430780887604,\n",
    " 0.1856636106967926,\n",
    " 0.18083277344703674,\n",
    " 0.17602944374084473,\n",
    " 0.17126891016960144,\n",
    " 0.1665983945131302,\n",
    " 0.1617734432220459,\n",
    " 0.15704314410686493,\n",
    " 0.15238535404205322,\n",
    " 0.14765579998493195,\n",
    " 0.14270250499248505,\n",
    " 0.13798731565475464,\n",
    " 0.13292498886585236,\n",
    " 0.12823647260665894,\n",
    " 0.12362143397331238,\n",
    " 0.11873777210712433,\n",
    " 0.11375843733549118,\n",
    " 0.10896897315979004,\n",
    " 0.10412222892045975,\n",
    " 0.09941406548023224,\n",
    " 0.0947309210896492,\n",
    " 0.08967708051204681,\n",
    " 0.0849035307765007,\n",
    " 0.07996275275945663,\n",
    " 0.07521139085292816,\n",
    " 0.0703568309545517,\n",
    " 0.06544049829244614,\n",
    " 0.060721978545188904,\n",
    " 0.055874887853860855,\n",
    " 0.05085190385580063,\n",
    " 0.04614328593015671,\n",
    " 0.04101845622062683,\n",
    " 0.03615574166178703,\n",
    " 0.031399454921483994,\n",
    " 0.026684783399105072,\n",
    " 0.021683650091290474,\n",
    " 0.01681867055594921,\n",
    " 0.011774173006415367,\n",
    " 0.0068734148517251015,\n",
    " 0.002297158120200038,\n",
    " -0.0030477242544293404,\n",
    " -0.007704370655119419,\n",
    " -0.012447916902601719,\n",
    " -0.017642833292484283,\n",
    " -0.022409534081816673,\n",
    " -0.027348166331648827,\n",
    " -0.0320054329931736,\n",
    " -0.03722361847758293,\n",
    " -0.04239923134446144,\n",
    " -0.047066010534763336,\n",
    " -0.05229789391160011,\n",
    " -0.05706952139735222,\n",
    " -0.06205932796001434,\n",
    " -0.06680095195770264,\n",
    " -0.0717138797044754,\n",
    " -0.07664956897497177,\n",
    " -0.08159057796001434,\n",
    " -0.08668705075979233,\n",
    " -0.09163597971200943,\n",
    " -0.09644429385662079,\n",
    " -0.10136514902114868,\n",
    " -0.10637997835874557,\n",
    " -0.11145210266113281,\n",
    " -0.11633820086717606,\n",
    " -0.12122305482625961,\n",
    " -0.12639957666397095,\n",
    " -0.13123664259910583,\n",
    " -0.13601437211036682,\n",
    " -0.14116224646568298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f10497c9da0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.axes._base:update_title_pos\n",
      "DEBUG:matplotlib.axes._base:update_title_pos\n",
      "DEBUG:matplotlib.axes._base:update_title_pos\n",
      "DEBUG:matplotlib.axes._base:update_title_pos\n",
      "DEBUG:matplotlib.axes._base:update_title_pos\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_200_RBF,linewidth=2 ,label='RBF Kernel')\n",
    "plt.plot(loss_200_RQ, linewidth=2 ,label='RQ Kernel')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
